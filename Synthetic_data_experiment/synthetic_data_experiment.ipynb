{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a03f08-39a1-4526-af38-f49f0430baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b5a0cb-c960-4b20-83cd-3bf681dcc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed= 1729           #1729  \n",
    "pyro_seed=  68      #68  \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyro\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(pyro_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121ae98-54cb-41c2-9857-40298650bfe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86395961-586b-4367-b7a5-af0ad8bcd10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "\n",
    "import pandas as pd\n",
    "import scipy.misc as m\n",
    "from math import isnan, isinf\n",
    "\n",
    "import shutil\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "\n",
    "from torch.distributions import Independent\n",
    "from pyro.infer.reparam.transform import TransformReparam\n",
    "from pyro.distributions.torch_transform import ComposeTransformModule\n",
    "from pyro.nn import PyroModule, pyro_method\n",
    "from pyro.distributions import Normal,TransformedDistribution\n",
    "from pyro.distributions.transforms import Spline,affine_coupling,conditional_affine_coupling, affine_autoregressive,conditional_affine_autoregressive, AffineTransform, ExpTransform, ComposeTransform, spline_coupling \n",
    "from pyro.distributions.transforms import conditional_spline\n",
    "from pyro.distributions.conditional import ConditionalTransformedDistribution\n",
    "\n",
    "\n",
    "from utils import mkdir\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7500f5-7504-4e19-922d-eb9b14049aa7",
   "metadata": {},
   "source": [
    "### *Structural Causal Model*\n",
    "$X_{4}=2\\epsilon_{4}+1$\n",
    "<br>\n",
    "$X_{6}=\\epsilon_{6}-1$\n",
    "<br>\n",
    "$X_{5}=3X_{6}+\\epsilon_{5}-1$\n",
    "<br>\n",
    "$X_{2}=X_{5}-\\epsilon_{2}$\n",
    "<br>\n",
    "$X_{3}=-3X_{4}+\\epsilon_{3}-3$\n",
    "<br>\n",
    "$X_{1}=X_{6}-X_{5}+3\\epsilon_{1}$\n",
    "<br>\n",
    "$Y=X_{1}+2X_{2}-3X_{3}+\\epsilon_{Y}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acb8c77-e984-4aff-8261-496be7918fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=10000\n",
    "epsilon=np.random.normal(0,1,(sample_size,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8e9ddf-cfec-48ea-8570-b35a4b438a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCM(epsilon,intervention=False, *args):\n",
    "    X_4=2*epsilon[:,4]+1\n",
    "    X_6=epsilon[:,6]-1\n",
    "    if not intervention:\n",
    "        X_5=3*X_6+epsilon[:,5]-1\n",
    "    else:\n",
    "        X_5=np.full(sample_size,args)\n",
    "    X_2=X_5-epsilon[:,2]\n",
    "    X_3=-3*X_4+epsilon[:,3]-3\n",
    "    X_1=X_6-X_5+3*epsilon[:,1]\n",
    "    Y=X_1+2*X_2-3*X_3+epsilon[:,0]\n",
    "    return X_6,X_5,X_4,X_3,X_2,X_1,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65be3da8-e426-4969-9680-1da3652bd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_6,X_5,X_4,X_3,X_2,X_1,Y= SCM(epsilon,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98db6bb8-f3cb-4c26-90fc-dcb1c464981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data={'X6':torch.tensor(X_6).reshape(-1,1).float(),'X5':torch.tensor(X_5).reshape(-1,1).float(),'X4':torch.tensor(X_4).reshape(-1,1).float(),'X3':torch.tensor(X_3).reshape(-1,1).float(),'X2':torch.tensor(X_2).reshape(-1,1).float(),'X1':torch.tensor(X_1).reshape(-1,1).float(),'Y':torch.tensor(Y).reshape(-1,1).float()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6533951-0095-4fb9-9092-53c78d7eb961",
   "metadata": {},
   "source": [
    "### *Preprocessing the data*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204e082b-938f-4d84-994d-99954871f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class featuresDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,dim = 1,noise=epsilon,\n",
    "        inference = False, train = True,\n",
    "        train_ratio = 0.8\n",
    "        #random_seed = 67,\n",
    "    ):\n",
    "        super(featuresDataset, self).__init__()\n",
    "        self.dim = dim\n",
    "        #self.random_seed =   69 #random_seed\n",
    "        self.inference = inference\n",
    "        self.noise=noise\n",
    "\n",
    "        self.data, self.y = [], []\n",
    "        print('==> started preprocessing data ...')\n",
    "        #X_6,X_5,X_4,X_3,X_2,X_1,Y= SCM(epsilon,False)\n",
    "        #print(Y[0:5])\n",
    "        for index in range(sample_size):\n",
    "            self.data.append(dict(\n",
    "                X6 =  np.asarray(X_6[index]).astype(np.float32),\n",
    "                X5 =  np.asarray(X_5[index]).astype(np.float32),\n",
    "                X4 =  np.asarray(X_4[index]).astype(np.float32),\n",
    "                X3 =  np.asarray(X_3[index]).astype(np.float32),\n",
    "                X2 =  np.asarray(X_2[index]).astype(np.float32),\n",
    "                X1 =  np.asarray(X_1[index]).astype(np.float32),\n",
    "                Y  =  np.asarray(Y[index]).astype(np.float32), \n",
    "                sample_id= index\n",
    "            ))\n",
    "        print('==> finished preprocessing data ...')\n",
    "\n",
    "        if not self.inference:                                                      #when inference is flase \n",
    "            self.ids = np.arange(0, len(self.data))           #img_idxes --> ids\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(self.ids)\n",
    "            last_train_sample = int(len(self.ids) * train_ratio)\n",
    "            if train:                                                               #when training is true\n",
    "                self.ids = self.ids[:last_train_sample]\n",
    "            else:\n",
    "                self.ids = self.ids[last_train_sample:]\n",
    "        else:                                                                       #when inference is true\n",
    "            self.ids = np.arange(0, len(self.data))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id_ = self.ids[index]\n",
    "        info = self.data[id_]\n",
    "        \n",
    "        id= info['sample_id']\n",
    "        X6 = torch.from_numpy(info['X6'])\n",
    "        X5 = torch.from_numpy(info['X5'])\n",
    "        X4 = torch.from_numpy(info['X4'])\n",
    "        X3 = torch.from_numpy(info['X3'])\n",
    "        X2 = torch.from_numpy(info['X2'])\n",
    "        X1 = torch.from_numpy(info['X1'])\n",
    "        Y = torch.from_numpy(info['Y'])\n",
    "        #X4 = torch.tensor().float()\n",
    "        #age = torch.tensor(info['age']).unsqueeze(-1).float()\n",
    "        \n",
    "\n",
    "        if self.inference:\n",
    "            return X6, X5, X4,  X3, X2,  X1, Y, id\n",
    "        else:\n",
    "            return X6, X5, X4,  X3, X2,  X1, Y\n",
    "\n",
    "\n",
    "def get_features_dataset(\n",
    "    dim=1,\n",
    "    inference=False,\n",
    "    train_ratio=0.8,\n",
    "    noise=epsilon\n",
    "    #random_seed=67,\n",
    "    ):\n",
    "\n",
    "    if not inference:\n",
    "        dataset_train = featuresDataset(train=True, dim=dim,noise=epsilon,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "        dataset_test = featuresDataset(train=False,  dim=dim,noise=epsilon,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "\n",
    "        return dataset_train, dataset_test\n",
    "    else:\n",
    "        dataset = featuresDataset(inference=True,  dim=dim,noise=epsilon,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2b9d6-8dbb-4683-b22a-9a05b60f98ce",
   "metadata": {},
   "source": [
    "### Flow based SCMs\n",
    "---\n",
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d7f0ce7-932a-4962-8850-a3501f9c4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full model\n",
    "\n",
    "class ConditionalSCM(PyroModule):\n",
    "    def __init__(self,context_dim=2,normalize=True,spline_bins=8,spline_order='linear'):\n",
    "        super(ConditionalSCM, self).__init__()\n",
    "        self.context_dim = context_dim\n",
    "        self.spline_bins = spline_bins\n",
    "        self.spline_order = spline_order\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "        \n",
    "        #Flows ---------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # X_4 flows\n",
    "        self.X4_flow_component =  affine_coupling(1)                         #Spline(1, count_bins=self.spline_bins)\n",
    "        self.X4_flow_transforms = ComposeTransform([self.X4_flow_component])\n",
    "        \n",
    "        # X_6 flows\n",
    "        self.X6_flow_component =   affine_coupling(1)                        #Spline(1, count_bins=self.spline_bins)\n",
    "        self.X6_flow_transforms = ComposeTransform([self.X6_flow_component])\n",
    "        \n",
    "        #X_5 flows\n",
    "        self.X5_flow_component =   conditional_affine_coupling(input_dim=1, context_dim=1,hidden_dims=[10,10])  #conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X5_flow_transforms = ComposeTransformModule([self.X5_flow_component])\n",
    "        \n",
    "        #X_2 flows \n",
    "        self.X2_flow_component =     conditional_affine_coupling(input_dim=1, context_dim=1,hidden_dims=[10,10]) #conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X2_flow_transforms = ComposeTransformModule([self.X2_flow_component])\n",
    "        \n",
    "        #X_3 flows \n",
    "        self.X3_flow_component =     conditional_affine_coupling(input_dim=1, context_dim=1,hidden_dims=[10,10])                         #conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X3_flow_transforms = ComposeTransformModule([self.X3_flow_component])\n",
    "        \n",
    "        \n",
    "        #X_1 flows \n",
    "        self.X1_flow_component =     conditional_affine_coupling(input_dim=1, context_dim=2,hidden_dims=[10,10])          #conditional_spline(1, context_dim=2, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X1_flow_transforms = ComposeTransformModule([self.X1_flow_component])\n",
    "        \n",
    "        #Y flows\n",
    "        self.Y_flow_component =  conditional_affine_coupling(input_dim=1, context_dim=3,hidden_dims=[10,10])                                        #conditional_spline(1, context_dim=3, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.Y_flow_transforms = ComposeTransformModule([self.Y_flow_component])\n",
    "        \n",
    "       \n",
    "\n",
    "            \n",
    "    def pgm_model(self,**kwargs):\n",
    "        loc=torch.tensor([0.0])\n",
    "        scale=torch.tensor([1.0])\n",
    "        \n",
    "        #observed={}\n",
    "        #if kwargs:\n",
    "        #    for key, value in kwargs.items():\n",
    "        #        observed[key]=value\n",
    "        #else:\n",
    "        #    observed={'X4':None,'X6':None,'X5':None,'X3':None,'X2':None,'X1':None,'Y':None}\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # X4\n",
    "        self.X4_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X4_dist=TransformedDistribution(self.X4_base_dist, self.X4_flow_transforms)\n",
    "        self.X4 = pyro.sample('X4', self.X4_dist,) #obs=observed['X4']\n",
    "        # X6\n",
    "        self.X6_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X6_dist=TransformedDistribution(self.X6_base_dist, self.X6_flow_transforms)\n",
    "        self.X6 = pyro.sample('X6', self.X6_dist,) #obs=observed['X6']\n",
    "        #X5\n",
    "        context_X5=torch.cat([self.X6],-1)\n",
    "        self.X5_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X5_dist = ConditionalTransformedDistribution(self.X5_base_dist, self.X5_flow_transforms)\n",
    "        self.X5 = pyro.sample('X5', self.X5_dist.condition(context_X5),) #obs=observed['X5']\n",
    "        #X3\n",
    "        context_X3=torch.cat([self.X4],-1)\n",
    "        self.X3_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X3_dist = ConditionalTransformedDistribution(self.X3_base_dist, self.X3_flow_transforms)\n",
    "        self.X3 = pyro.sample('X3', self.X3_dist.condition(context_X3),) #obs=observed['X3']\n",
    "        #X2\n",
    "        context_X2=torch.cat([self.X5],-1)\n",
    "        self.X2_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X2_dist = ConditionalTransformedDistribution(self.X2_base_dist, self.X2_flow_transforms)\n",
    "        self.X2 = pyro.sample('X2', self.X2_dist.condition(context_X2),) #obs=observed['X2']\n",
    "        #X1\n",
    "        context_X1=torch.cat([self.X5,self.X6],-1)\n",
    "        #print('context_X1',context_X1)\n",
    "        self.X1_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X1_dist = ConditionalTransformedDistribution(self.X1_base_dist, self.X1_flow_transforms)\n",
    "        self.X1 = pyro.sample('X1', self.X1_dist.condition(context_X1),) #obs=observed['X1']\n",
    "        # Y\n",
    "        context_Y=torch.cat([self.X3,self.X2,self.X1],-1)\n",
    "        self.Y_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.Y_dist = ConditionalTransformedDistribution(self.Y_base_dist, self.Y_flow_transforms)\n",
    "        self.Y = pyro.sample('Y', self.Y_dist.condition(context_Y),) #obs=observed['Y']\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X6,X5, X4,X3,X2,X1,Y ):\n",
    "        self.pgm_model() #**{\"X6\":X6,\"X5\": X5,\"X4\": X4,\"X3\":X3,\"X2\":X2,\"X1\":X1,\"Y\":Y})\n",
    "        \n",
    "        \n",
    "                \n",
    " \n",
    "        \n",
    "        \n",
    "        context_X5=torch.cat([X6],-1).detach()\n",
    "        context_X3=torch.cat([X4],-1).detach()\n",
    "        context_X2=torch.cat([X5],-1).detach()\n",
    "        context_X1=torch.cat([X5,X6],-1).detach()\n",
    "        context_Y=torch.cat([X3,X2,X1],-1).detach()\n",
    "         \n",
    "        X6_logp = self.X6_dist.log_prob(X6)\n",
    "        X5_logp = self.X5_dist.condition(context_X5).log_prob(X5)\n",
    "        X4_logp = self.X4_dist.log_prob(X4)\n",
    "        X3_logp = self.X3_dist.condition(context_X3).log_prob(X3)\n",
    "        X2_logp = self.X2_dist.condition(context_X2).log_prob(X2)\n",
    "        X1_logp = self.X1_dist.condition(context_X1).log_prob(X1)\n",
    "        Y_logp = self.Y_dist.condition(context_Y).log_prob(Y)\n",
    "        \n",
    "        return {'X_6':X6_logp,'X_5':X5_logp,'X_4': X4_logp, 'X_3': X3_logp, 'X_2': X2_logp, 'X_1': X1_logp,'Y': Y_logp}\n",
    "\n",
    "    def clear(self):\n",
    "        self.X5_dist.clear_cache()\n",
    "        self.X3_dist.clear_cache()\n",
    "        self.X2_dist.clear_cache()\n",
    "        self.X1_dist.clear_cache()\n",
    "        self.Y_dist.clear_cache()\n",
    "        \n",
    "    def model(self):\n",
    "        self.pgm_model()\n",
    "        return self.X6, self.X5, self.X4, self.X3, self.X2, self.X1, self.Y\n",
    "\n",
    "    def scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam()\n",
    "            else:\n",
    "                return None\n",
    "        return pyro.poutine.reparam(self.model, config=config)(*args, **kwargs)\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('sample_observations', n_samples):\n",
    "            samples = self.model()\n",
    "        return (*samples,)\n",
    "\n",
    "    def sample_scm(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.scm()\n",
    "        return (*samples,)\n",
    "    \n",
    "    def infer_exogeneous(self, **obs):\n",
    "        cond_sample = pyro.condition(self.sample, data=obs)\n",
    "        cond_trace = pyro.poutine.trace(cond_sample).get_trace(obs['X1'].shape[0])\n",
    "        output = {}\n",
    "        for name, node in cond_trace.nodes.items():\n",
    "            if 'fn' not in node.keys():\n",
    "                continue\n",
    "            fn = node['fn']\n",
    "            if isinstance(fn, Independent):\n",
    "                fn = fn.base_dist\n",
    "\n",
    "            if isinstance(fn, TransformedDistribution):\n",
    "                output[name + '_base'] = ComposeTransform(fn.transforms).inv(node['value'])\n",
    "        return output\n",
    "\n",
    "    def counterfactual(self, obs,intervention, num_particles=1):\n",
    "        counterfactuals = []\n",
    "\n",
    "        for _ in range(num_particles):\n",
    "            exogeneous = self.infer_exogeneous(**obs)\n",
    "            condition= {intervention.split('=')[0].split('(')[-1] :torch.full_like( obs['X4'],float(intervention.split('=')[-1][:-1]))}\n",
    "            counter = pyro.poutine.do(pyro.poutine.condition(self.sample_scm, data=exogeneous), data=condition)(obs['X1'].shape[0]) \n",
    "            counterfactuals += [counter]\n",
    "            #print('helper',helper)\n",
    "            #print('helper',helper)\n",
    "\n",
    "        return {k: v for k, v in zip(('X_6','X_5','X_4', 'X_3', 'X_2', 'X_1','Y'), (torch.stack(c).mean(0) for c in zip(*counterfactuals)))}\n",
    "\n",
    "\n",
    "def conditionalscm():\n",
    "    model = ConditionalSCM(context_dim=2,normalize=True,spline_bins=16,spline_order='linear')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8992360e-6c94-4e6f-90ed-5d7159ef9f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saptarshi/anaconda3/envs/flow-scm/lib/python3.9/site-packages/torch/nn/init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "scm=conditionalscm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1307b31-66e6-4044-881b-60cc9c92e6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a5489e3-12c6-45ba-95d9-3255e96f9b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scm.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ceceb80-51f4-4065-a90d-2d36f6bf5866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ = {'X6': torch.rand(1).reshape(-1,1),\n",
    "#        'X5': torch.rand(1).reshape(-1,1),\n",
    "#        'X4': torch.rand(1).reshape(-1,1), \n",
    "#        'X3': torch.rand(1).reshape(-1,1), \n",
    "#        'X2': torch.rand(1).reshape(-1,1),\n",
    "#        'X1': torch.rand(1).reshape(-1,1),\n",
    "#        'Y': torch.rand(1).reshape(-1,1)} \n",
    "#print(data_)\n",
    "#data2 = {'X6': torch.rand(2).reshape(-1,1),\n",
    "#        'X5': torch.rand(2).reshape(-1,1),\n",
    "#        'X4': torch.rand(2).reshape(-1,1), \n",
    "#        'X3': torch.rand(2).reshape(-1,1), \n",
    "#        'X2': torch.rand(2).reshape(-1,1),\n",
    "#        'X1': torch.rand(2).reshape(-1,1),\n",
    "#        'Y': torch.rand(2).reshape(-1,1)} \n",
    "#print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93387ea9-79bf-42c5-b3cc-29b69b980ce3",
   "metadata": {},
   "source": [
    "Note that, in the full model, we infer state of all $\\epsilon_{i}$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6502de58-e731-406a-acdb-cf3c4511f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scm.infer_exogeneous(**data_))\n",
    "#exogeneous=scm.infer_exogeneous(**data_)\n",
    "#print(scm.infer_exogeneous(**data2))\n",
    "#exogeneous=scm.infer_exogeneous(**data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7999e60d-6ce7-410c-98c2-c6156cfe7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intervention='do(X5=0.7)'\n",
    "#print(scm.counterfactual(obs=data2,intervention=intervention))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aa3e0-ccba-4235-b453-79c7b8681951",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b24e80-8d4e-4431-b51e-5b06b2855a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partial model\n",
    "\n",
    "class ConditionalSCM_partial(PyroModule):\n",
    "    def __init__(self,context_dim=2,normalize=True,spline_bins=8,spline_order='linear'):\n",
    "        super(ConditionalSCM_partial, self).__init__()\n",
    "        self.context_dim = context_dim\n",
    "        self.spline_bins = spline_bins\n",
    "        self.spline_order = spline_order\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "        \n",
    "        #Flows ---------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # X_4 flows\n",
    "        #self.X4_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        #self.X4_flow_transforms = ComposeTransform([self.X4_flow_component])\n",
    "        \n",
    "        # X_3 flows\n",
    "        #self.X3_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        #self.X3_flow_transforms = ComposeTransform([self.X3_flow_component])\n",
    "        \n",
    "        #X_2 flows \n",
    "        self.X2_flow_component =  conditional_affine_coupling(input_dim=1, context_dim=1,hidden_dims=[10,10])  #conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X2_flow_transforms = ComposeTransformModule([self.X2_flow_component])\n",
    "        \n",
    "        #X_1 flows \n",
    "        self.X1_flow_component = conditional_affine_coupling(input_dim=1, context_dim=2,hidden_dims=[10,10])  #conditional_spline(1, context_dim=2, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X1_flow_transforms = ComposeTransformModule([self.X1_flow_component])\n",
    "        \n",
    "        #Y flows\n",
    "        self.Y_flow_component = conditional_affine_coupling(input_dim=1, context_dim=3,hidden_dims=[10,10])  #conditional_spline(1, context_dim=3, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.Y_flow_transforms = ComposeTransformModule([self.Y_flow_component])\n",
    "        \n",
    "       \n",
    "\n",
    "            \n",
    "    def pgm_model(self,**kwargs):\n",
    "        loc=torch.tensor([0.0])\n",
    "        scale=torch.tensor([1.0])\n",
    "        \n",
    "        #observed={}\n",
    "        #if kwargs:\n",
    "        #    for key, value in kwargs.items():\n",
    "        #        observed[key]=value\n",
    "        #else:\n",
    "        #    observed={'X4':None,'X6':None,'X5':None,'X3':None,'X2':None,'X1':None,'Y':None}\n",
    "        \n",
    "        \n",
    "         # X4\n",
    "        self.X4_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X4_dist=TransformedDistribution(self.X4_base_dist, self.X4_flow_transforms)\n",
    "        self.X4 = pyro.sample('X4', self.X4_dist,) #obs=observed['X4'])\n",
    "        # X6\n",
    "        self.X6_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X6_dist=TransformedDistribution(self.X6_base_dist, self.X6_flow_transforms)\n",
    "        self.X6 = pyro.sample('X6', self.X6_dist,)#obs=observed['X6'])\n",
    "        #X5\n",
    "        #context_X5=torch.cat([self.X6],-1)\n",
    "        self.X5_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X5_dist = ConditionalTransformedDistribution(self.X5_base_dist, self.X5_flow_transforms).condition(context_X5)\n",
    "        self.X5 = pyro.sample('X5', self.X5_dist,)#obs=observed['X5'])\n",
    "        #X3\n",
    "        #context_X3=torch.cat([self.X4],-1)\n",
    "        self.X3_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X3_dist = ConditionalTransformedDistribution(self.X3_base_dist, self.X3_flow_transforms).condition(context_X3)\n",
    "        self.X3 = pyro.sample('X3', self.X3_dist,)#obs=observed['X3'])\n",
    "        \n",
    "        #X2\n",
    "        context_X2=torch.cat([self.X5],-1)\n",
    "        self.X2_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X2_dist = ConditionalTransformedDistribution(self.X2_base_dist, self.X2_flow_transforms)\n",
    "        self.X2 = pyro.sample('X2', self.X2_dist.condition(context_X2),)#obs=observed['X2'])\n",
    "        #X1\n",
    "        context_X1=torch.cat([self.X5,self.X6],-1)\n",
    "        self.X1_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X1_dist = ConditionalTransformedDistribution(self.X1_base_dist, self.X1_flow_transforms)\n",
    "        self.X1 = pyro.sample('X1', self.X1_dist.condition(context_X1),)#obs=observed['X1'])\n",
    "        # Y\n",
    "        context_Y=torch.cat([self.X3,self.X2,self.X1],-1)\n",
    "        self.Y_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.Y_dist = ConditionalTransformedDistribution(self.Y_base_dist, self.Y_flow_transforms)\n",
    "        self.Y = pyro.sample('Y', self.Y_dist.condition(context_Y),)#obs=observed['Y'])\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X6, X5, X4,X3,X2,X1,Y ):\n",
    "        self.pgm_model() #**{\"X6\":X6,\"X5\": X5,\"X4\": X4,\"X3\":X3,\"X2\":X2,\"X1\":X1,\"Y\":Y})\n",
    "        \n",
    "        \n",
    "        \n",
    "        context_X2=torch.cat([X5],-1).detach()\n",
    "        context_X1=torch.cat([X5,X6],-1).detach()\n",
    "        context_Y=torch.cat([X3,X2,X1],-1).detach()        \n",
    "        \n",
    "        Y_logp = self.Y_dist.condition(context_Y).log_prob(Y)\n",
    "        X1_logp = self.X1_dist.condition(context_X1).log_prob(X1)\n",
    "        X2_logp = self.X2_dist.condition(context_X2).log_prob(X2)\n",
    "        \n",
    "        X3_logp = torch.zeros(Y_logp.shape) #.detach()                                  #self.X3_dist.log_prob(X3)                  \n",
    "        X4_logp = torch.zeros(Y_logp.shape) #.detach()                                  #self.X4_dist.log_prob(X4)                                \n",
    "        X5_logp = torch.zeros(Y_logp.shape) #.detach()                                  #self.X2_dist.log_prob(X2)\n",
    "        X6_logp = torch.zeros(Y_logp.shape) #.detach()                                  #self.X1_dist.log_prob(X1)\n",
    "        \n",
    "        return {'X_6': X6_logp,'X_5': X5_logp,'X_4': X4_logp, 'X_3': X3_logp, 'X_2': X2_logp, 'X_1': X1_logp,'Y': Y_logp}\n",
    "\n",
    "    def clear(self):\n",
    "        self.X2_dist.clear_cache()\n",
    "        self.X1_dist.clear_cache()\n",
    "        self.Y_dist.clear_cache()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def partial_model(self,**obs):\n",
    "        self.pgm_model()\n",
    "        \n",
    "        X4=  obs['X4']       \n",
    "        X6 = obs['X6']                \n",
    "        X5 = obs['X5']                   \n",
    "        X3 = obs['X3']\n",
    "        context_X2=torch.cat([X5],-1)\n",
    "        X2 = pyro.sample('X2', self.X2_dist.condition(context_X2),)\n",
    "        context_X1=torch.cat([X5,X6],-1)\n",
    "        X1 = pyro.sample('X1', self.X1_dist.condition(context_X1),)\n",
    "        context_Y=torch.cat([X3,X2,X1],-1)\n",
    "        Y = pyro.sample('Y', self.Y_dist.condition(context_Y),) \n",
    "\n",
    "        return  X6, X5, X4, X3, X2, X1, Y\n",
    "        \n",
    "    def partial_sample(self,**observed):\n",
    "        with pyro.plate('partial_observation', observed['X4'].shape[0]):\n",
    "          samples = self.partial_model(**observed)\n",
    "        return (*samples,)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        self.pgm_model()\n",
    "        return self.X6, self.X5, self.X4, self.X3, self.X2, self.X1, self.Y\n",
    "\n",
    "    def scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam()\n",
    "            else:\n",
    "                return None\n",
    "        return pyro.poutine.reparam(self.model, config=config)(*args, **kwargs)\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('sample_observations', n_samples):\n",
    "            samples = self.model()\n",
    "        return (*samples,)\n",
    "\n",
    "    def sample_scm(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.scm()\n",
    "        return (*samples,)\n",
    "    \n",
    "    def infer_exogeneous(self, **obs):\n",
    "        cond_sample = pyro.condition(self.sample, data=obs)\n",
    "        cond_trace = pyro.poutine.trace(cond_sample).get_trace(obs['X1'].shape[0])\n",
    "        output = {}\n",
    "        for name, node in cond_trace.nodes.items():\n",
    "            if 'fn' not in node.keys():\n",
    "                continue\n",
    "            fn = node['fn']\n",
    "            if isinstance(fn, Independent):\n",
    "                fn = fn.base_dist\n",
    "\n",
    "            if isinstance(fn, TransformedDistribution):\n",
    "                output[name + '_base'] = ComposeTransform(fn.transforms).inv(node['value'])\n",
    "        return output\n",
    "\n",
    "    def counterfactual(self, obs, intervention, num_particles=1):\n",
    "        counterfactuals = []\n",
    "        for _ in range(num_particles):\n",
    "            exogeneous = self.infer_exogeneous(**obs)\n",
    "            condition= {intervention.split('=')[0].split('(')[-1] :torch.full_like( obs['X4'],float(intervention.split('=')[-1][:-1]))}\n",
    "            condition['X3']=obs['X3']\n",
    "            condition['X4']=obs['X4']\n",
    "            condition['X6']=obs['X6']\n",
    "            #print('condition',condition)\n",
    "            counter = pyro.poutine.do(pyro.poutine.condition(self.sample_scm, data=exogeneous), data=condition)(obs['X1'].shape[0])\n",
    "            counterfactuals += [counter]\n",
    "        return {k: v for k, v in zip(('X_6','X_5','X_4', 'X_3', 'X_2', 'X_1','Y'), (torch.stack(c).mean(0) for c in zip(*counterfactuals)))}\n",
    "\n",
    "\n",
    "def conditionalscm_partial():\n",
    "    model = ConditionalSCM_partial(context_dim=2,normalize=True,spline_bins=16,spline_order='linear')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37007e68-b967-43f9-b57f-2e0fa30b1576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e3bae85-a5ab-461e-9545-1070e83c7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_partial=conditionalscm_partial()\n",
    "#print(scm_partial.sample(n_samples=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaae213-b4db-44f7-aeb0-8e04ac7f6038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b52f35f4-73f8-43eb-8509-39789565ca92",
   "metadata": {},
   "source": [
    "Note that we only infer $\\epsilon_{2},\\epsilon_{1}$ and $\\epsilon_{Y}$. We hard intervene on $X_{5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "349aaa71-eb88-4305-8bd6-7cdfa824e2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scm_partial.infer_exogeneous(**data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1164abb-94cf-4a77-bbad-4e2b8b0cc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intervention='do(X5=3)'\n",
    "#print(scm_partial.counterfactual(obs=data_,intervention=intervention))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f6509-43a0-43d6-9ac1-34b658ee1052",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training function \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb588b-fc65-4f20-9c27-151830bc61aa",
   "metadata": {},
   "source": [
    "**training hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad1124e7-37c3-42b1-8da3-4a87fbeec017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_seed=69    #default 69\n",
    "batch_size=128\n",
    "test_batch_size=64\n",
    "lr=1e-3  #0.1            #3e-4\n",
    "weight_decay=0     #1e-3\n",
    "epochs=1000\n",
    "lr_annealing=True\n",
    "log_interval=100\n",
    "#seed=17        #default 17\n",
    "save='./logs'    #filepath to save training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a2f0e-b454-4362-b812-1bc3e5d4f86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d70f7-f36d-424e-b017-913b45ed1c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51851a1e-813c-4584-9f21-303d009c1e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def milestone_step( optimizer, epoch):\n",
    "    if epoch in [epochs*0.5, epochs*0.75]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f86606-c659-4903-8d0d-522d3bb144a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d265ed-3e1c-446c-8981-3f5426720f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a241c90-11eb-4ffd-b5ef-5cc41d9302e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "#import torch\n",
    "import shutil\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "from utils import mkdir\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filepath):\n",
    "    mkdir(filepath)\n",
    "    torch.save(state, os.path.join(filepath, 'flow_ckpt.pth.tar'))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(filepath, 'flow_ckpt.pth.tar'), os.path.join(filepath, 'flow_best.pth.tar'))\n",
    "        #print('best is saved')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def train( model, optimizer, train_loader, epoch):\n",
    "    avg_loss = 0.\n",
    "    for batch_idx, (X6,X5,X4,X3, X2, X1,Y) in enumerate(train_loader):\n",
    "        X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "        optimizer.zero_grad()\n",
    "        log_p = model(X6, X5, X4, X3 ,X2, X1,Y)\n",
    "        loss = -torch.mean(log_p['X_6']+log_p['X_5']+log_p['X_4']+log_p['X_3']+log_p['X_2']+log_p['X_1'] +log_p['Y'])\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.clear()\n",
    "        #if epoch % log_interval == 0:\n",
    "            #print('Train Epoch: {} [{}/{} ({:.1f}%)]\\t| -LogProb X6: {:.6f}\\tX5: {:.6f}\\tX4: {:.6f}\\tX3: {:.6f} \\tX2: {:.6f}\\tX1: {:.6f} \\tY: {:.6f} \\tTotal logprob: {:.6f}'.format(epoch, batch_idx * len(X1),\n",
    "                #len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                #-torch.mean(log_p['X_6']).item(), -torch.mean(log_p['X_5']).item(),\n",
    "                #-torch.mean(log_p['X_4']).item(), -torch.mean(log_p['X_3']).item(),\n",
    "                #-torch.mean(log_p['X_2']).item(), -torch.mean(log_p['X_1']).item(),-torch.mean(log_p['Y']).item(), -loss.item()))\n",
    "    avg_loss /= -len(train_loader)\n",
    "    if epoch % log_interval ==0:\n",
    "        print('Epoch:',epoch,'\\nTrain set: Average LogProb: {:.6f}\\n'.format(avg_loss))\n",
    "    \n",
    "            \n",
    "            \n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss = 0.\n",
    "    model.eval()\n",
    "    associative_power ={'X_6':0,'X_5':0,'X_4':0,'X_3':0,'X_2':0,'X_1':0,'Y':0}\n",
    "    for X6, X5, X4, X3, X2, X1,Y in test_loader:\n",
    "        with torch.no_grad():\n",
    "            X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "            log_p = model(X6, X5, X4, X3 ,X2, X1,Y)\n",
    "            test_loss += torch.mean(log_p['X_6']+log_p['X_5']+log_p['X_4']+log_p['X_3']+log_p['X_2']+log_p['X_1'] +log_p['Y'])\n",
    "            for k,v in log_p.items():\n",
    "                associative_power[k]+=torch.mean(v)\n",
    "                #print(k,v)\n",
    "                \n",
    "                \n",
    "    #print('length',len(test_loader))            \n",
    "    test_loss /= len(test_loader)   #len(test_loader.dataset)\n",
    "    associative_power=dict(associative_power) \n",
    "    associative_power={k: v/len(test_loader) for k,v in associative_power.items()}\n",
    "    #print('associative power',associative_power, 'sum',sum(associative_power.values()))\n",
    "    #print('\\nTest set: Average LogProb: {:.6f}\\n'.format(test_loss))\n",
    "    #print('--------------------------------------------------')\n",
    "    return test_loss, associative_power\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "def main(model_name,filepath):\n",
    "    kwargs =  {}\n",
    "    seed=21\n",
    "    dataset_train, dataset_test = get_features_dataset( dim=1,noise=epsilon) #random_seed=data_seed)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model = model_name\n",
    " \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 0)\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min') \n",
    "\n",
    "    best_loss = -500.\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    " \n",
    "        #if epoch %7 ==2:\n",
    "            \n",
    "        train( model, optimizer, train_loader, epoch)\n",
    "        loss,ass_power = test(model, test_loader)\n",
    "        if epoch%100==0:\n",
    "            print('\\nTest set: Average LogProb: {:.6f}\\n'.format(loss))\n",
    "        if not lr_annealing:\n",
    "            milestone_step(optimizer, epoch)\n",
    "        else:\n",
    "            scheduler.step() #loss)\n",
    "        \n",
    "        is_best = loss > best_loss\n",
    "        best_loss = max(loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'associative power':ass_power\n",
    "        }, is_best, filepath=filepath)\n",
    "\n",
    "    del model\n",
    "    print('==> Best LogProb: {:.6f}, Time: {:.2f} min\\n'.format(best_loss, (time.time()-start_time)/60.))\n",
    "    \n",
    "\n",
    "    \n",
    "def train_model(model,path):\n",
    "    mkdir(path)\n",
    "    path = os.path.join(path, str(model.__class__.__name__))\n",
    "    mkdir(path)\n",
    "    filepath=path\n",
    "    main(model,filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f7217-49b7-4de7-b834-e829ebfcd612",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train scm \n",
    "---\n",
    "We haven't trained the models yet. We need to train them to get proper counterfactuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b9d8da8-b6c6-4f67-b777-bab1bb399d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "test_batch_size=64\n",
    "lr=1e-3  #0.1            #3e-4\n",
    "weight_decay=0    #1e-3\n",
    "epochs=1000\n",
    "lr_annealing=True\n",
    "log_interval=100\n",
    "save='./logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cef3f9-1832-439a-94b5-a55e1e6d2939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "Epoch: 0 \n",
      "Train set: Average LogProb: -272.027583\n",
      "\n",
      "\n",
      "Test set: Average LogProb: -127.593544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(scm,save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3868fb-b71d-48a0-9f36-31da63c86419",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train scm_partial\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46cd0f7-784f-424f-8060-d684f794ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "test_batch_size=64\n",
    "lr=1e-3  #0.1            #3e-4\n",
    "weight_decay=0    #1e-3\n",
    "epochs=1000\n",
    "lr_annealing=True\n",
    "log_interval=100\n",
    "save='./logs' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadff18-0b6c-4eed-be45-ed19dc4ac167",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(scm_partial,save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19377ff-f6d9-4722-9d8c-9a9d1da91b01",
   "metadata": {},
   "source": [
    "## Goodness of Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ae2b6-4aaa-4fdf-ac8a-671954df773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525656e6-ee35-47cb-bd95-ba06762bea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scm_sample=scm.sample(n_samples=2000)\n",
    "#scm_sample={'X6':scm_sample[0].detach(),'X5':scm_sample[1].detach(),'X4':scm_sample[2].detach(),'X3':scm_sample[3].detach(),'X2':scm_sample[4].detach(),'X1':scm_sample[5].detach(),'Y':scm_sample[6].detach()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c4abdc-97c5-4702-95fc-670d572e3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scm_partial_sample=scm_partial.sample(n_samples=2000)\n",
    "#scm_partial_sample={'X6':scm_partial_sample[0].detach(),'X5':scm_partial_sample[1].detach(),'X4':scm_partial_sample[2].detach(),'X3':scm_partial_sample[3].detach(),'X2':scm_partial_sample[4].detach(),'X1':scm_partial_sample[5].detach(),'Y':scm_partial_sample[6].detach()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb02a436-a060-4ae7-8ce9-1bf76b33617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(Data['Y'].numpy(),kde=True,color='red')\n",
    "#sns.distplot(scm_sample['Y'].numpy(),kde=True,color='yellow')\n",
    "#sns.distplot(scm_partial_sample['Y'].numpy(),kde=True,color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfb6a4-1721-42ee-a37e-2f0501039b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inferred_noise_scm=scm.infer_exogeneous(**Data)\n",
    "#inferred_noise_scm_partial=scm_partial.infer_exogeneous(**Data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7cf439-b9f6-4be4-9dc3-01c458403593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.distplot(inferred_noise_scm['Y_base'].detach(),color='yellow')\n",
    "#sns.distplot(inferred_noise_scm_partial['Y_base'].detach(),color='green')\n",
    "#sns.distplot(epsilon[:,0],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2430cd-3b28-4b50-85b1-6f822e636f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#j=10\n",
    "#intervention='do(X5='+str(j)+')'\n",
    "#scm_partial_counterfactuals=scm_partial.counterfactual(obs=Data,intervention=intervention)\n",
    "#scm_counterfactuals=scm.counterfactual(obs=Data,intervention=intervention)\n",
    "#X6_cf,X5_cf, X4_cf, X3_cf, X2_cf, X1_cf, Y_cf= SCM(epsilon,True,j) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b671f6a-0934-4dc7-bf8d-2d18c45b512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.distplot(scm_partial_counterfactuals['Y'].detach(),color='green')\n",
    "#sns.distplot(scm_counterfactuals['Y'].detach(),color='yellow')\n",
    "#sns.distplot( Y_cf,color='red')\n",
    "#sns.distplot(epsilon[:,1],color='gray')\n",
    "#sns.distplot(inferred_noise_scm['X1_base'].detach(),color='orange')\n",
    "#sns.distplot(inferred_noise_scm_partial['X1_base'].detach(),color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5426d69-16ea-479d-8492-070bdb6fb285",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Inference**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274aa80-18c6-46cc-8eb0-3bbb25733f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(model_name,intervention,noise=epsilon):\n",
    "    #kwargs = {}\n",
    "    #dataset = get_features_dataset(inference=True, dim=dim,noise=epsilon) #random_seed=9)\n",
    "    #loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    model = model_name\n",
    "    saved_in_path= os.path.join(save,str(model_name.__class__.__name__)) \n",
    "    model_path = os.path.join(saved_in_path, 'flow_best.pth.tar')\n",
    "    checkpoint = torch.load(model_path)\n",
    "    print('\\n',model_path)\n",
    "    print('====>>> Best LogProb: {:.6f}\\n'.format(checkpoint['best_loss']))\n",
    "    print('associative power:',checkpoint['associative power'])\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print('intervention------',intervention)\n",
    "    #model.clear()\n",
    "    #feature = FeaturesInference(args, model, loader, dataset.data, dataset.covariates_dict)\n",
    "    #feature.inference()\n",
    "    #Counterfactuals={'X_6':[],'X_5':[],'X_4':[],'X_3':[],'X_2':[],'X_1':[],'Y':[]}\n",
    "    #print('before starting---',Counterfactuals)\n",
    "    #start_time=time.time()\n",
    "    #for batch_idx, (X6, X5, X4, X3, X2, X1,Y,id) in enumerate(loader):\n",
    "            #with torch.no_grad():\n",
    "                #X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "                #data = {'X6': X6,'X5': X5,'X4': X4, 'X3': X3, 'X2': X2, 'X1': X1,'Y':Y}\n",
    "                #print('intervention',intervention)\n",
    "    if 'partial' in model.__class__.__name__:\n",
    "        Samples=model.partial_sample(n_samples=1000,**Data)  \n",
    "        samples={'X6':Samples[0].detach(),'X5':Samples[1].detach(),'X4':Samples[2].detach(),'X3':Samples[3].detach(),'X2':Samples[4].detach(),'X1':Samples[5].detach(),'Y':Samples[6].detach()}\n",
    "    else:\n",
    "        Samples=model.sample(n_samples=sample_size)  \n",
    "        samples={'X6':Samples[0].detach(),'X5':Samples[1].detach(),'X4':Samples[2].detach(),'X3':Samples[3].detach(),'X2':Samples[4].detach(),'X1':Samples[5].detach(),'Y':Samples[6].detach()}\n",
    "\n",
    "    Exogeneous_noise=model.infer_exogeneous(**Data)\n",
    "    Counterfactuals = model.counterfactual(obs=Data, intervention=intervention) #, num_particles=particles)\n",
    "                #if batch_idx % log_interval == 0:\n",
    "                    #print('Inference: [{}/{} ({:.1f}%)]\\tIntervention: {}'.format(\n",
    "                        #batch_idx * len(X4), len(loader.dataset), 100. * batch_idx / len(loader), intervention))\n",
    "                    #print(samples)\n",
    "                #for key,value in Counterfactuals.items():\n",
    "                    #Counterfactuals[key] .append(np.array(samples[key]))\n",
    "\n",
    "    #print(' Time taken: {:.2f} secs'.format( (time.time()-start_time)))\n",
    "    \n",
    "    #for key,value in Counterfactuals.items():\n",
    "        #Counterfactuals[key]= np.vstack(Counterfactuals[key]).reshape(sample_size) \n",
    "    \n",
    "    return samples ,Exogeneous_noise, Counterfactuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b765c-15ba-438a-966b-e6a2520c8f76",
   "metadata": {},
   "source": [
    "### *Generate counterfactuals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d537363-fd09-4006-82b0-42c71cdc7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bae61e-7634-4e57-b2c7-49cb98b1b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "j= 5\n",
    "intervention='do(X5='+str(j)+')'\n",
    "X6_cf,X5_cf, X4_cf, X3_cf, X2_cf, X1_cf, Y_cf= SCM(epsilon,True,j)      #True Counterfactuals\n",
    "print(np.mean(Y_cf))\n",
    "#print(Y_cf[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7f088-a21e-432f-b8f6-4a2eac0f0b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCM_samples,SCM_inferred_noise,SCM_counterfactuals=do_inference(scm,intervention,noise=epsilon)\n",
    "#np.mean(SCM_counterfactuals['Y'])\n",
    "#SCM_counterfactuals['Y'].detach().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d514ad2-3a83-4140-8d74-22c5f72067bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44402f82-daa1-4468-a380-6c34a19733af",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCM_partial_samples,SCM_partial_inferred_noise,SCM_partial_counterfactuals=do_inference(scm_partial,intervention,noise=epsilon)\n",
    "#print(SCM_partial_counterfactuals['Y'].detach().mean())\n",
    "#SCM_partial_counterfactuals['Y'].detach().numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e42c9e-c64f-4014-b8de-9fdba43e6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_estimate(inferred_noise):\n",
    "    dict={}\n",
    "    for key,value in inferred_noise.items():\n",
    "        t=key.split('_')[0][-1]\n",
    "        if t=='Y':\n",
    "            s=0\n",
    "        else:\n",
    "            s=int(t)\n",
    "        dict[key.split('_')[0]+' error']=np.linalg.norm(inferred_noise[key].detach().numpy().reshape(-1)-epsilon[:,s])/sample_size\n",
    "    return dict\n",
    "\n",
    "print('full model:', error_estimate(SCM_inferred_noise))\n",
    "print('\\npartial model:', error_estimate(SCM_partial_inferred_noise))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4c3ee-e39b-4cbe-91c3-0725b3eb6f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(Data['Y'].numpy(),kde=True,color='red')\n",
    "#sns.distplot(SCM_samples['Y'].numpy().reshape(-1),kde=True,color='yellow')\n",
    "#sns.distplot(SCM_partial_samples['Y'].numpy().reshape(-1),kde=True,color='black')\n",
    "\n",
    "\n",
    "\n",
    "sample_Y={'True':Data['Y'].reshape(-1),'full':SCM_samples['Y'].detach().numpy().reshape(-1),'partial':SCM_partial_samples['Y'].detach().numpy().reshape(-1)}\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(24, 6))\n",
    "sns.histplot(sample_Y['True'], ax=ax1, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_Y['partial'], ax=ax1,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_Y['full'], ax=ax1,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_ylabel(' ',fontweight='bold')\n",
    "ax1.set_title(r'$Y$',fontsize=15)\n",
    "\n",
    "sample_X1={'True':Data['X1'].reshape(-1),'full':SCM_samples['X1'].detach().numpy().reshape(-1),'partial':SCM_partial_samples['X1'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(sample_X1['True'], ax=ax2, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X1['partial'], ax=ax2,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X1['full'], ax=ax2,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_ylabel(' ',fontweight='bold')\n",
    "ax2.set_title(r'$X_{1}$',fontsize=15)\n",
    "\n",
    "\n",
    "sample_X2={'True':Data['X2'].reshape(-1),'full':SCM_samples['X2'].detach().numpy().reshape(-1),'partial':SCM_partial_samples['X2'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(sample_X2['True'], ax=ax3, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X2['partial'], ax=ax3,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X2['full'], ax=ax3,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax3.legend()\n",
    "ax3.set_ylabel(' ',fontweight='bold')\n",
    "ax3.set_title(r'$X_{2}$',fontsize=15)\n",
    "\n",
    "\n",
    "fig.savefig('sampling_capabilities.pdf',format='pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb443c-fe1d-484a-be47-f47d522c8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0,ax1,ax2,ax3) = plt.subplots(1,4,figsize=(24, 6))\n",
    "\n",
    "\n",
    "sample_X3={'True':Data['X3'].reshape(-1),'full':SCM_samples['X3'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(sample_X3['True'], ax=ax0, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(sample_Y['partial'], ax=ax1,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X3['full'], ax=ax0,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax0.legend()\n",
    "ax0.set_ylabel(' ',fontweight='bold')\n",
    "ax0.set_title(r'$X_{3}$',fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "sample_X4={'True':Data['X4'].reshape(-1),'full':SCM_samples['X4'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(sample_X4['True'], ax=ax1, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(sample_Y['partial'], ax=ax1,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X4['full'], ax=ax1,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_ylabel(' ',fontweight='bold')\n",
    "ax1.set_title(r'$X_{4}$',fontsize=15)\n",
    "\n",
    "sample_X5={'True':Data['X5'].reshape(-1),'full':SCM_samples['X5'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(sample_X5['True'], ax=ax2, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(sample_X5['partial'], ax=ax2,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X5['full'], ax=ax2,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_ylabel(' ',fontweight='bold')\n",
    "ax2.set_title(r'$X_{5}$',fontsize=15)\n",
    "\n",
    "\n",
    "sample_X6={'True':Data['X6'].reshape(-1),'full':SCM_samples['X6'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(sample_X6['True'], ax=ax3, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(sample_X6['partial'], ax=ax3,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(sample_X6['full'], ax=ax3,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax3.legend()\n",
    "ax3.set_ylabel(' ',fontweight='bold')\n",
    "ax3.set_title(r'$X_{6}$',fontsize=15)\n",
    "\n",
    "\n",
    "fig.savefig('additional_sampling_capabilities.pdf',format='pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167ffc45-d451-4479-a3ed-65a9e53c5940",
   "metadata": {},
   "source": [
    "## Noise Inferrence Capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fea5e-8751-457e-8382-c5c90969ce81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9988e4a8-91fe-4571-a668-c9b5118611d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_noise_Y={'True':epsilon[:,0].reshape(-1),'partial':SCM_partial_inferred_noise['Y_base'].detach().numpy().reshape(-1),'full':SCM_inferred_noise['Y_base'].detach().numpy().reshape(-1)}\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3,figsize=(24, 6))\n",
    "sns.histplot(infer_noise_Y['True'], ax=ax1, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_Y['partial'], ax=ax1,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_Y['full'], ax=ax1,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_ylabel(' ',fontweight='bold')\n",
    "ax1.set_title(r'$\\epsilon_{Y}$',fontsize=15)\n",
    "\n",
    "infer_noise_X1={'True':epsilon[:,1].reshape(-1),'partial':SCM_partial_inferred_noise['X1_base'].detach().numpy().reshape(-1),'full':SCM_inferred_noise['X1_base'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(infer_noise_X1['True'], ax=ax2, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X1['partial'], ax=ax2,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X1['full'], ax=ax2,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_ylabel(' ',fontweight='bold')\n",
    "ax2.set_title(r'$\\epsilon_{1}$',fontsize=15)\n",
    "\n",
    "\n",
    "infer_noise_X2={'True':epsilon[:,2].reshape(-1),'partial':SCM_partial_inferred_noise['X2_base'].detach().numpy().reshape(-1),'full':SCM_inferred_noise['X2_base'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(infer_noise_X2['True'], ax=ax3, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X2['partial'], ax=ax3,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X2['full'], ax=ax3,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax3.legend()\n",
    "ax3.set_ylabel(' ',fontweight='bold')\n",
    "ax3.set_title(r'$\\epsilon_{2}$',fontsize=15)\n",
    "\n",
    "\n",
    "fig.savefig('inferred_noises.pdf',format='pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bcb5cd-b002-4e5b-9813-8a270bfb2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_noise_X3={'True':epsilon[:,3].reshape(-1),'full':SCM_inferred_noise['X3_base'].detach().numpy().reshape(-1)}\n",
    "fig, (ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize=(24, 5))\n",
    "sns.histplot(infer_noise_X3['True'], ax=ax1, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(infer_noise_Y['partial'], ax=ax1,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X3['full'], ax=ax1,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax1.legend()\n",
    "ax1.set_ylabel(' ',fontweight='bold')\n",
    "ax1.set_title(r'$\\epsilon_{3}$',fontsize=15)\n",
    "\n",
    "infer_noise_X4={'True':epsilon[:,4].reshape(-1),'full':SCM_inferred_noise['X4_base'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(infer_noise_X4['True'], ax=ax2, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(infer_noise_X1['partial'], ax=ax2,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X4['full'], ax=ax2,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax2.legend()\n",
    "ax2.set_ylabel(' ',fontweight='bold')\n",
    "ax2.set_title(r'$\\epsilon_{4}$',fontsize=15)\n",
    "\n",
    "\n",
    "infer_noise_X5={'True':epsilon[:,5].reshape(-1),'full':SCM_inferred_noise['X5_base'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(infer_noise_X5['True'], ax=ax3, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(infer_noise_X2['partial'], ax=ax3,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X5['full'], ax=ax3,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax3.legend()\n",
    "ax3.set_ylabel(' ',fontweight='bold')\n",
    "ax3.set_title(r'$\\epsilon_{5}$',fontsize=15)\n",
    "\n",
    "infer_noise_X6={'True':epsilon[:,6].reshape(-1),'full':SCM_inferred_noise['X6_base'].detach().numpy().reshape(-1)}\n",
    "sns.histplot(infer_noise_X6['True'], ax=ax4, color='red',legend=True,label='True',alpha=0.2,kde=True)\n",
    "#sns.histplot(infer_noise_X2['partial'], ax=ax3,color='black',legend=True,label='partial model',alpha=0.2,kde=True)\n",
    "sns.histplot(infer_noise_X6['full'], ax=ax4,color='green',legend=True,label='full model',alpha=0.2,kde=True)\n",
    "\n",
    "ax4.legend()\n",
    "ax4.set_ylabel(' ',fontweight='bold')\n",
    "ax4.set_title(r'$\\epsilon_{6}$',fontsize=15)\n",
    "\n",
    "\n",
    "fig.savefig('additional_inferred_noises.pdf',format='pdf',dpi=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdba04a3-47b2-45a3-9d8e-efc771a65262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.displot(SCM_inferred_noise['Y_base'].detach().numpy().reshape(-1),color='yellow',legend=True)\n",
    "#sns.displot(SCM_partial_inferred_noise['Y_base'].detach().numpy().reshape(-1),color='black',legend=True)\n",
    "#sns.displot(epsilon[:,0].reshape(-1),color='red',alpha=0.1,legend=True)\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a5ef4-77a1-4a5d-97f1-a443b379fef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7def7c9-312b-471e-967f-676c98e62d0e",
   "metadata": {},
   "source": [
    "Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85245f31-7dca-4b64-8eef-a39526f3166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_style('darkgrid')\n",
    "data={'Y':Y_cf,'partial':SCM_partial_counterfactuals['Y'].detach().numpy().reshape(-1),'full':SCM_counterfactuals['Y'].detach().numpy().reshape(-1)}\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.kdeplot(data['Y'], ax=ax, color='red', fill=True,bw_adjust=2,legend=True,label='True',alpha=0.1)\n",
    "sns.kdeplot(data['full'], ax=ax, color='green',linestyle='-' ,fill=True,bw_adjust=2,legend=True,label='full model')\n",
    "sns.kdeplot(data['partial'], ax=ax,color='black',linestyle='--' ,fill=True,bw_adjust=2,legend=True,label='partial model')\n",
    "plt.legend()\n",
    "plt.ylabel('Density',fontweight='bold')\n",
    "plt.title(r'$P(Y_{X_{5}\\leftarrow %.2f}|\\mathbf{X}=\\mathbf{x}^{obs},Y=y^{obs})$'%j,fontsize=15)\n",
    "fig.savefig('toy_model2.pdf',format='pdf',dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae11ae-f0e6-4fd6-be8d-19a61ce57c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5354f68c-0595-44fa-b56d-baa59359f225",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba069cf-595c-4025-b7f1-57c4d27ceca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "intervention_set=np.random.uniform(-30,30,200)\n",
    "error_full=[]\n",
    "error_partial=[]\n",
    "\n",
    "for j in intervention_set:\n",
    "    intervention='do(X5='+str(j)+')'\n",
    "    SCM_samples,SCM_inferred_noise,SCM_counterfactuals=do_inference(scm,intervention,noise=epsilon)\n",
    "    SCM_partial_samples,SCM_partial_inferred_noise,SCM_partial_counterfactuals=do_inference(scm_partial,intervention,noise=epsilon)\n",
    "    \n",
    "    X6_cf, X5_cf, X4_cf, X3_cf, X2_cf, X1_cf, Y_cf= SCM(epsilon,True,j)\n",
    "    \n",
    "    error_full.append( np.linalg.norm(Y_cf.reshape(-1)-SCM_counterfactuals['Y'].detach().numpy().reshape(-1))/(sample_size))\n",
    "    error_partial.append(np.linalg.norm(Y_cf.reshape(-1)-SCM_partial_counterfactuals['Y'].detach().numpy().reshape(-1))/(sample_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94a68d-e73d-4240-b821-fcf76bf73852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({'intervention':intervention_set,'error_partial':error_partial,'error_full': error_full})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08999493-472b-450e-8506-8d683fe458d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "#plt.scatter(intervention_set,np.exp(np.array(error_partial)),label='partial model',color='black',ls='-',marker='o')\n",
    "#plt.scatter(intervention_set,np.exp(np.array(error_full)),label='full model',color='yellow',ls='--',marker='*')\n",
    "sns.lineplot(data=data,x='intervention',y='error_partial',label='partial model',color='black',marker='o',alpha=0.5)\n",
    "sns.lineplot(data=data,x='intervention',y='error_full',label='full model',color='green',marker='*',alpha=0.5)\n",
    "\n",
    "plt.xlabel(r'Intervention value on $X_{5}$ ',fontweight='bold')\n",
    "plt.ylabel('mean squared error',fontweight='bold')\n",
    "plt.legend()\n",
    "fig.savefig('error_synthetic_data.pdf',format='pdf',dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f26bd5-88f1-4842-8b08-5d0123a2bb1b",
   "metadata": {},
   "source": [
    "## Results on unseen data from the SCM\n",
    "We don't train the SCM with these data points ....we just do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625edac-09c1-4406-80f6-b28191cbc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=89\n",
    "np.random.seed(seed)\n",
    "sample_size=10000\n",
    "epsilon_unseen=np.random.normal(0,1,(sample_size,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bd4ba7-1edc-4c28-b331-ddc2c1cfa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_6,X_5,X_4,X_3,X_2,X_1,Y= SCM(epsilon_unseen,False)\n",
    "Data={'X6':torch.tensor(X_6).reshape(-1,1).float(),'X5':torch.tensor(X_5).reshape(-1,1).float(),'X4':torch.tensor(X_4).reshape(-1,1).float(),'X3':torch.tensor(X_3).reshape(-1,1).float(),'X2':torch.tensor(X_2).reshape(-1,1).float(),'X1':torch.tensor(X_1).reshape(-1,1).float(),'Y':torch.tensor(Y).reshape(-1,1).float()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef3fe7-2ae7-40c6-865c-e9343c7e3cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "j= 1\n",
    "intervention='do(X5='+str(j)+')'\n",
    "X6_cf,X5_cf, X4_cf, X3_cf, X2_cf, X1_cf, Y_cf= SCM(epsilon_unseen,True,j)      #True Counterfactuals\n",
    "#np.mean(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773feec0-e6cd-44b7-9bb2-9cb9beaa10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCM_samples,SCM_inferred_noise,SCM_counterfactuals=do_inference(scm,intervention,noise=epsilon_unseen)\n",
    "#np.mean(SCM_counterfactuals['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c39798-b5e7-49bd-bcf9-c6ba32d8de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCM_partial_samples,SCM_partial_inferred_noise,SCM_partial_counterfactuals=do_inference(scm_partial,intervention,noise=epsilon_unseen)\n",
    "#np.mean(SCM_partial_counterfactuals['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0ab14-1d8b-4636-b259-ab27d11235df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Data['Y'].numpy(),kde=True,color='red')\n",
    "sns.distplot(SCM_samples['Y'].numpy().reshape(-1),kde=True,color='green')\n",
    "sns.distplot(SCM_partial_samples['Y'].numpy().reshape(-1),kde=True,color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0902f634-dff6-45fc-9871-262445acb7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(SCM_inferred_noise['Y_base'].detach(),color='green')\n",
    "sns.distplot(SCM_partial_inferred_noise['Y_base'].detach(),color='black')\n",
    "sns.distplot(epsilon_unseen[:,0],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dc69aa-3c1f-47b5-acaa-1c4290a2bfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b4411c-bdbb-4cf2-ab62-96f431e683a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "#sns.set_style('darkgrid')\n",
    "\n",
    "data={'Y':Y_cf,'partial':SCM_partial_counterfactuals['Y'].detach().numpy().reshape(-1),'full':SCM_counterfactuals['Y'].detach().numpy().reshape(-1)}\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.kdeplot(data['Y'], ax=ax, color='red', fill=True,bw_adjust=2,legend=True,label='True',alpha=0.1)\n",
    "sns.kdeplot(data['full'], ax=ax, color='green',linestyle='-' ,fill=True,bw_adjust=2,legend=True,label='full model')\n",
    "sns.kdeplot(data['partial'], ax=ax,color='black',linestyle='-' ,fill=True,bw_adjust=2,legend=True,label='partial model')\n",
    "plt.legend()\n",
    "plt.ylabel('Density',fontweight='bold')\n",
    "plt.title(r'$P(Y_{X_{5}\\leftarrow %.2f}|\\mathbf{X}=\\mathbf{x}^{obs},Y=y^{obs})$'%j,fontsize=15)\n",
    "#fig.savefig('toy_model_unseen_data.pdf',format='pdf',dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2961e7b2-de55-499a-92e5-96d7e273ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervention_set=np.random.uniform(-30,30,200)\n",
    "error_full=[]\n",
    "error_partial=[]\n",
    "\n",
    "for j in intervention_set:\n",
    "    intervention='do(X5='+str(j)+')'\n",
    "    SCM_samples,SCM_inferred_noise,SCM_counterfactuals=do_inference(scm,intervention,noise=epsilon)\n",
    "    SCM_partial_samples,SCM_partial_inferred_noise,SCM_partial_counterfactuals=do_inference(scm_partial,intervention,noise=epsilon)\n",
    "    \n",
    "    X6_cf, X5_cf, X4_cf, X3_cf, X2_cf, X1_cf, Y_cf= SCM(epsilon,True,j)\n",
    "    \n",
    "    error_full.append( np.linalg.norm(Y_cf.reshape(-1)-SCM_counterfactuals['Y'].detach().numpy().reshape(-1))/sample_size)\n",
    "    error_partial.append(np.linalg.norm(Y_cf.reshape(-1)-SCM_partial_counterfactuals['Y'].detach().numpy().reshape(-1))/sample_size)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a43510-ca3e-4ab2-8bd0-682235c35373",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({'intervention':intervention_set,'error_partial':error_partial,'error_full': error_full})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1d26b-84ca-4537-96fd-fe5c674bfacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "#plt.scatter(intervention_set,np.exp(np.array(error_partial)),label='partial model',color='black',ls='-',marker='o')\n",
    "#plt.scatter(intervention_set,np.exp(np.array(error_full)),label='full model',color='yellow',ls='--',marker='*')\n",
    "sns.lineplot(data=data,x='intervention',y='error_partial',label='partial model',color='black',marker='o',alpha=0.5)\n",
    "sns.lineplot(data=data,x='intervention',y='error_full',label='full model',color='green',marker='*',alpha=0.5)\n",
    "\n",
    "plt.xlabel(r'Intervention value on $X_{5}$ ',fontweight='bold')\n",
    "plt.ylabel('mean squared error',fontweight='bold')\n",
    "plt.legend()\n",
    "fig.savefig('testerror_synthetic_data.pdf',format='pdf',dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
