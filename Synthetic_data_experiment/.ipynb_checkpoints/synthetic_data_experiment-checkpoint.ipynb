{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4a03f08-39a1-4526-af38-f49f0430baa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(93)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7500f5-7504-4e19-922d-eb9b14049aa7",
   "metadata": {},
   "source": [
    "### *Structural Causal Model*\n",
    "$X_{4}=2\\epsilon_{4}+1$\n",
    "<br>\n",
    "$X_{6}=\\epsilon_{6}-1$\n",
    "<br>\n",
    "$X_{5}=3X_{6}+\\epsilon_{5}-1$\n",
    "<br>\n",
    "$X_{2}=X_{5}-\\epsilon_{2}$\n",
    "<br>\n",
    "$X_{3}=-3X_{4}+\\epsilon_{3}-3$\n",
    "<br>\n",
    "$X_{1}=X_{6}-X_{5}+3\\epsilon_{1}$\n",
    "<br>\n",
    "$Y=X_{1}+2X_{2}-3X_{3}+\\epsilon_{Y}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4acb8c77-e984-4aff-8261-496be7918fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=1000\n",
    "epsilon=np.random.normal(0,1,(sample_size,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8e9ddf-cfec-48ea-8570-b35a4b438a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCM(intervention=False,*args):\n",
    "    X_4=2*epsilon[:,4]+1\n",
    "    X_6=epsilon[:,6]-1\n",
    "    if not intervention:\n",
    "        X_5=3*X_6+epsilon[:,5]-1\n",
    "    else:\n",
    "        X_5=np.full(sample_size,args)\n",
    "    X_2=X_5-epsilon[:,2]\n",
    "    X_3=-3*X_4+epsilon[:,3]-3\n",
    "    X_1=X_6-X_5+3*epsilon[:,1]\n",
    "    Y=X_1+2*X_2-3*X_3+epsilon[:,0]\n",
    "    return X_6,X_5,X_4,X_3,X_2,X_1,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b5ae28-2714-45fa-8b2a-5fe49f0f10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_6,X_5,X_4,X_3,X_2,X_1,Y= SCM(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6533951-0095-4fb9-9092-53c78d7eb961",
   "metadata": {},
   "source": [
    "### *Preprocessing the data*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "204e082b-938f-4d84-994d-99954871f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.misc as m\n",
    "from torch.utils import data\n",
    "from math import isnan, isinf\n",
    "\n",
    "\n",
    "\n",
    "class featuresDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,dim = 1,\n",
    "        inference = False, train = True,\n",
    "        train_ratio = 0.8,\n",
    "        #random_seed = 67,\n",
    "    ):\n",
    "        super(featuresDataset, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.random_seed =   69 #random_seed\n",
    "        self.inference = inference\n",
    "\n",
    "        self.data, self.y = [], []\n",
    "        print('==> started preprocessing data ...')\n",
    "        for index in range(sample_size):\n",
    "            self.data.append(dict(\n",
    "                X6 =  np.asarray(X_6[index]).astype(np.float32),\n",
    "                X5 =  np.asarray(X_5[index]).astype(np.float32),\n",
    "                X4 =  np.asarray(X_4[index]).astype(np.float32),\n",
    "                X3 =  np.asarray(X_3[index]).astype(np.float32),\n",
    "                X2 =  np.asarray(X_2[index]).astype(np.float32),\n",
    "                X1 =  np.asarray(X_1[index]).astype(np.float32),\n",
    "                Y  =  np.asarray(Y[index]).astype(np.float32), \n",
    "                sample_id= index\n",
    "            ))\n",
    "        print('==> finished preprocessing data ...')\n",
    "\n",
    "        if not self.inference:                                                      #when inference is flase \n",
    "            self.ids = np.arange(0, len(self.data))           #img_idxes --> ids\n",
    "            np.random.seed(self.random_seed)\n",
    "            np.random.shuffle(self.ids)\n",
    "            last_train_sample = int(len(self.ids) * train_ratio)\n",
    "            if train:                                                               #when training is true\n",
    "                self.ids = self.ids[:last_train_sample]\n",
    "            else:\n",
    "                self.ids = self.ids[last_train_sample:]\n",
    "        else:                                                                       #when inference is true\n",
    "            self.ids = np.arange(0, len(self.data))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id_ = self.ids[index]\n",
    "        info = self.data[id_]\n",
    "        \n",
    "        id= info['sample_id']\n",
    "        X6 = torch.from_numpy(info['X6'])\n",
    "        X5 = torch.from_numpy(info['X5'])\n",
    "        X4 = torch.from_numpy(info['X4'])\n",
    "        X3 = torch.from_numpy(info['X3'])\n",
    "        X2 = torch.from_numpy(info['X2'])\n",
    "        X1 = torch.from_numpy(info['X1'])\n",
    "        Y = torch.from_numpy(info['Y'])\n",
    "        #X4 = torch.tensor().float()\n",
    "        #age = torch.tensor(info['age']).unsqueeze(-1).float()\n",
    "        \n",
    "\n",
    "        if self.inference:\n",
    "            return X6, X5, X4,  X3, X2,  X1, Y, id\n",
    "        else:\n",
    "            return X6, X5, X4,  X3, X2,  X1, Y\n",
    "\n",
    "\n",
    "def get_features_dataset(\n",
    "    dim=1,\n",
    "    inference=False,\n",
    "    train_ratio=0.8,\n",
    "    #random_seed=67,\n",
    "    ):\n",
    "\n",
    "    if not inference:\n",
    "        dataset_train = featuresDataset(train=True, dim=dim,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "        dataset_test = featuresDataset(train=False,  dim=dim,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "\n",
    "        return dataset_train, dataset_test\n",
    "    else:\n",
    "        dataset = featuresDataset(inference=True,  dim=dim,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122555a5-b790-470c-96e5-b9fa667cafb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### *Flow-based SCMs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5126c37b-da60-4445-aff2-ce59169d235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Independent\n",
    "from pyro.infer.reparam.transform import TransformReparam\n",
    "from pyro.distributions.torch_transform import ComposeTransformModule\n",
    "from pyro.nn import PyroModule, pyro_method\n",
    "from pyro.distributions import Normal,TransformedDistribution\n",
    "from pyro.distributions.transforms import Spline, AffineTransform, ExpTransform, ComposeTransform, spline_coupling \n",
    "from pyro.distributions.transforms import conditional_spline\n",
    "from pyro.distributions.conditional import ConditionalTransformedDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2b9d6-8dbb-4683-b22a-9a05b60f98ce",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d7f0ce7-932a-4962-8850-a3501f9c4385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full model\n",
    "\n",
    "class ConditionalSCM(PyroModule):\n",
    "    def __init__(self,context_dim=2,normalize=True,spline_bins=8,spline_order='linear'):\n",
    "        super(ConditionalSCM, self).__init__()\n",
    "        self.context_dim = context_dim\n",
    "        self.spline_bins = spline_bins\n",
    "        self.spline_order = spline_order\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "        \n",
    "        #Flows ---------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # X_4 flows\n",
    "        self.X4_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        self.X4_flow_transforms = ComposeTransform([self.X4_flow_component])\n",
    "        \n",
    "        # X_6 flows\n",
    "        self.X6_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        self.X6_flow_transforms = ComposeTransform([self.X6_flow_component])\n",
    "        \n",
    "        #X_5 flows\n",
    "        self.X5_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X5_flow_transforms = ComposeTransformModule([self.X5_flow_component])\n",
    "        \n",
    "        #X_2 flows \n",
    "        self.X2_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X2_flow_transforms = ComposeTransformModule([self.X2_flow_component])\n",
    "        \n",
    "        #X_3 flows \n",
    "        self.X3_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X3_flow_transforms = ComposeTransformModule([self.X3_flow_component])\n",
    "        \n",
    "        \n",
    "        #X_1 flows \n",
    "        self.X1_flow_component = conditional_spline(1, context_dim=2, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X1_flow_transforms = ComposeTransformModule([self.X1_flow_component])\n",
    "        \n",
    "        #Y flows\n",
    "        self.Y_flow_component = conditional_spline(1, context_dim=3, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.Y_flow_transforms = ComposeTransformModule([self.Y_flow_component])\n",
    "        \n",
    "       \n",
    "\n",
    "            \n",
    "    def pgm_model(self):\n",
    "        loc=torch.tensor([0.0])\n",
    "        scale=torch.tensor([1.0])\n",
    "        # X4\n",
    "        self.X4_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X4_dist=TransformedDistribution(self.X4_base_dist, self.X4_flow_transforms)\n",
    "        self.X4 = pyro.sample('X4', self.X4_dist,)\n",
    "        # X6\n",
    "        self.X6_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X6_dist=TransformedDistribution(self.X6_base_dist, self.X6_flow_transforms)\n",
    "        self.X6 = pyro.sample('X6', self.X6_dist,)\n",
    "        #X5\n",
    "        context_X5=torch.cat([self.X6],-1)\n",
    "        self.X5_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X5_dist = ConditionalTransformedDistribution(self.X5_base_dist, self.X5_flow_transforms).condition(context_X5)\n",
    "        self.X5 = pyro.sample('X5', self.X5_dist)\n",
    "        #X3\n",
    "        context_X3=torch.cat([self.X4],-1)\n",
    "        self.X3_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X3_dist = ConditionalTransformedDistribution(self.X3_base_dist, self.X3_flow_transforms).condition(context_X3)\n",
    "        self.X3 = pyro.sample('X3', self.X3_dist)\n",
    "        #X2\n",
    "        context_X2=torch.cat([self.X5],-1)\n",
    "        self.X2_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X2_dist = ConditionalTransformedDistribution(self.X2_base_dist, self.X2_flow_transforms).condition(context_X2)\n",
    "        self.X2 = pyro.sample('X2', self.X2_dist)\n",
    "        #X1\n",
    "        context_X1=torch.cat([self.X5,self.X6],-1)\n",
    "        self.X1_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X1_dist = ConditionalTransformedDistribution(self.X1_base_dist, self.X1_flow_transforms).condition(context_X1)\n",
    "        self.X1 = pyro.sample('X1', self.X1_dist)\n",
    "        # Y\n",
    "        context_Y=torch.cat([self.X3,self.X2,self.X1],-1)\n",
    "        self.Y_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.Y_dist = ConditionalTransformedDistribution(self.Y_base_dist, self.Y_flow_transforms).condition(context_Y)\n",
    "        self.Y = pyro.sample('Y', self.Y_dist)\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X6,X5, X4,X3,X2,X1,Y ):\n",
    "        self.pgm_model()\n",
    "        X6_logp = self.X6_dist.log_prob(X6)\n",
    "        X5_logp = self.X5_dist.log_prob(X5)\n",
    "        X4_logp = self.X4_dist.log_prob(X4)\n",
    "        X3_logp = self.X3_dist.log_prob(X3)\n",
    "        X2_logp = self.X2_dist.log_prob(X2)\n",
    "        X1_logp = self.X1_dist.log_prob(X1)\n",
    "        Y_logp = self.Y_dist.log_prob(Y)\n",
    "        return {'X_6':X6_logp,'X_5':X5_logp,'X_4': X4_logp, 'X_3': X3_logp, 'X_2': X2_logp, 'X_1': X1_logp,'Y': Y_logp}\n",
    "\n",
    "    def clear(self):\n",
    "        self.X5_dist.clear_cache()\n",
    "        self.X3_dist.clear_cache()\n",
    "        self.X2_dist.clear_cache()\n",
    "        self.X1_dist.clear_cache()\n",
    "        self.Y_dist.clear_cache()\n",
    "        \n",
    "    def model(self):\n",
    "        self.pgm_model()\n",
    "        return self.X6, self.X5, self.X4, self.X3, self.X2, self.X1, self.Y\n",
    "\n",
    "    def scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam()\n",
    "            else:\n",
    "                return None\n",
    "        return pyro.poutine.reparam(self.model, config=config)(*args, **kwargs)\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.model()\n",
    "        return (*samples,)\n",
    "\n",
    "    def sample_scm(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.scm()\n",
    "        return (*samples,)\n",
    "    \n",
    "    def infer_exogeneous(self, **obs):\n",
    "        cond_sample = pyro.condition(self.sample, data=obs)\n",
    "        cond_trace = pyro.poutine.trace(cond_sample).get_trace(obs['X1'].shape[0])\n",
    "        output = {}\n",
    "        for name, node in cond_trace.nodes.items():\n",
    "            if 'fn' not in node.keys():\n",
    "                continue\n",
    "            fn = node['fn']\n",
    "            if isinstance(fn, Independent):\n",
    "                fn = fn.base_dist\n",
    "\n",
    "            if isinstance(fn, TransformedDistribution):\n",
    "                output[name + '_base'] = ComposeTransform(fn.transforms).inv(node['value'])\n",
    "        return output\n",
    "\n",
    "    def counterfactual(self, obs,intervention, num_particles=1):\n",
    "        counterfactuals = []\n",
    "\n",
    "        for _ in range(num_particles):\n",
    "            exogeneous = self.infer_exogeneous(**obs)\n",
    "            condition= {intervention.split('=')[0].split('(')[-1] :torch.full_like( obs['X4'],float(intervention.split('=')[-1][:-1]))}\n",
    "            counter = pyro.poutine.do(pyro.poutine.condition(self.sample_scm, data=exogeneous), data=condition)(obs['X1'].shape[0])\n",
    "            counterfactuals += [counter]\n",
    "            #print('helper',helper)\n",
    "            #print('helper',helper)\n",
    "\n",
    "        return {k: v for k, v in zip(('X_6','X_5','X_4', 'X_3', 'X_2', 'X_1','Y'), (torch.stack(c).mean(0) for c in zip(*counterfactuals)))}\n",
    "\n",
    "\n",
    "def conditionalscm():\n",
    "    model = ConditionalSCM(context_dim=2,normalize=True,spline_bins=8,spline_order='linear')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8992360e-6c94-4e6f-90ed-5d7159ef9f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "scm=conditionalscm()\n",
    "#print(scm.sample(n_samples=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5489e3-12c6-45ba-95d9-3255e96f9b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ceceb80-51f4-4065-a90d-2d36f6bf5866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X6': tensor([[0.3960]]), 'X5': tensor([[0.1405]]), 'X4': tensor([[0.2559]]), 'X3': tensor([[0.3896]]), 'X2': tensor([[0.1750]]), 'X1': tensor([[0.3916]]), 'Y': tensor([[0.2014]])}\n",
      "{'X6': tensor([[0.6554],\n",
      "        [0.0393]]), 'X5': tensor([[0.4132],\n",
      "        [0.8467]]), 'X4': tensor([[0.8286],\n",
      "        [0.4782]]), 'X3': tensor([[0.8200],\n",
      "        [0.5977]]), 'X2': tensor([[0.7620],\n",
      "        [0.7924]]), 'X1': tensor([[0.8985],\n",
      "        [0.5049]]), 'Y': tensor([[0.4255],\n",
      "        [0.1001]])}\n"
     ]
    }
   ],
   "source": [
    "data = {'X6': torch.rand(1).reshape(-1,1),\n",
    "        'X5': torch.rand(1).reshape(-1,1),\n",
    "        'X4': torch.rand(1).reshape(-1,1), \n",
    "        'X3': torch.rand(1).reshape(-1,1), \n",
    "        'X2': torch.rand(1).reshape(-1,1),\n",
    "        'X1': torch.rand(1).reshape(-1,1),\n",
    "        'Y': torch.rand(1).reshape(-1,1)} \n",
    "print(data)\n",
    "data2 = {'X6': torch.rand(2).reshape(-1,1),\n",
    "        'X5': torch.rand(2).reshape(-1,1),\n",
    "        'X4': torch.rand(2).reshape(-1,1), \n",
    "        'X3': torch.rand(2).reshape(-1,1), \n",
    "        'X2': torch.rand(2).reshape(-1,1),\n",
    "        'X1': torch.rand(2).reshape(-1,1),\n",
    "        'Y': torch.rand(2).reshape(-1,1)} \n",
    "print(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93387ea9-79bf-42c5-b3cc-29b69b980ce3",
   "metadata": {},
   "source": [
    "Note that, in the full model, we infer state of all $\\epsilon_{i}$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6502de58-e731-406a-acdb-cf3c4511f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X4_base': tensor([[-2.5118]], grad_fn=<IndexPutBackward0>), 'X6_base': tensor([[-0.9081]], grad_fn=<IndexPutBackward0>), 'X5_base': tensor([[-0.3090]], grad_fn=<IndexPutBackward0>), 'X3_base': tensor([[0.2725]], grad_fn=<IndexPutBackward0>), 'X2_base': tensor([[0.1922]], grad_fn=<IndexPutBackward0>), 'X1_base': tensor([[0.2638]], grad_fn=<IndexPutBackward0>), 'Y_base': tensor([[0.4540]], grad_fn=<IndexPutBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(scm.infer_exogeneous(**data))\n",
    "exogeneous=scm.infer_exogeneous(**data)\n",
    "#print(scm.infer_exogeneous(**data2))\n",
    "#exogeneous=scm.infer_exogeneous(**data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7999e60d-6ce7-410c-98c2-c6156cfe7cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_6': tensor([[0.6554],\n",
      "        [0.0393]]), 'X_5': tensor([[0.7000],\n",
      "        [0.7000]]), 'X_4': tensor([[0.8286],\n",
      "        [0.4782]]), 'X_3': tensor([[0.8200],\n",
      "        [0.5977]], grad_fn=<MeanBackward1>), 'X_2': tensor([[0.7921],\n",
      "        [0.7784]], grad_fn=<MeanBackward1>), 'X_1': tensor([[0.9125],\n",
      "        [0.5007]], grad_fn=<MeanBackward1>), 'Y': tensor([[0.4209],\n",
      "        [0.1021]], grad_fn=<MeanBackward1>)}\n"
     ]
    }
   ],
   "source": [
    "intervention='do(X5=0.7)'\n",
    "print(scm.counterfactual(obs=data2,intervention=intervention))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905aa3e0-ccba-4235-b453-79c7b8681951",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b24e80-8d4e-4431-b51e-5b06b2855a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#partial model\n",
    "\n",
    "class ConditionalSCM_partial(PyroModule):\n",
    "    def __init__(self,context_dim=2,normalize=True,spline_bins=8,spline_order='linear'):\n",
    "        super(ConditionalSCM_partial, self).__init__()\n",
    "        self.context_dim = context_dim\n",
    "        self.spline_bins = spline_bins\n",
    "        self.spline_order = spline_order\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "        \n",
    "        #Flows ---------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # X_4 flows\n",
    "        #self.X4_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        #self.X4_flow_transforms = ComposeTransform([self.X4_flow_component])\n",
    "        \n",
    "        # X_3 flows\n",
    "        #self.X3_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        #self.X3_flow_transforms = ComposeTransform([self.X3_flow_component])\n",
    "        \n",
    "        #X_2 flows \n",
    "        self.X2_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X2_flow_transforms = ComposeTransformModule([self.X2_flow_component])\n",
    "        \n",
    "        #X_1 flows \n",
    "        self.X1_flow_component = conditional_spline(1, context_dim=2, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X1_flow_transforms = ComposeTransformModule([self.X1_flow_component])\n",
    "        \n",
    "        #Y flows\n",
    "        self.Y_flow_component = conditional_spline(1, context_dim=3, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.Y_flow_transforms = ComposeTransformModule([self.Y_flow_component])\n",
    "        \n",
    "       \n",
    "\n",
    "            \n",
    "    def pgm_model(self):\n",
    "        loc=torch.tensor([0.0])\n",
    "        scale=torch.tensor([1.0])\n",
    "         # X4\n",
    "        self.X4_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X4_dist=TransformedDistribution(self.X4_base_dist, self.X4_flow_transforms)\n",
    "        self.X4 = pyro.sample('X4', self.X4_dist,)\n",
    "        # X6\n",
    "        self.X6_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X6_dist=TransformedDistribution(self.X6_base_dist, self.X6_flow_transforms)\n",
    "        self.X6 = pyro.sample('X6', self.X6_dist,)\n",
    "        #X5\n",
    "        #context_X5=torch.cat([self.X6],-1)\n",
    "        self.X5_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X5_dist = ConditionalTransformedDistribution(self.X5_base_dist, self.X5_flow_transforms).condition(context_X5)\n",
    "        self.X5 = pyro.sample('X5', self.X5_dist)\n",
    "        #X3\n",
    "        #context_X3=torch.cat([self.X4],-1)\n",
    "        self.X3_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X3_dist = ConditionalTransformedDistribution(self.X3_base_dist, self.X3_flow_transforms).condition(context_X3)\n",
    "        self.X3 = pyro.sample('X3', self.X3_dist)\n",
    "        \n",
    "        #X2\n",
    "        context_X2=torch.cat([self.X5],-1)\n",
    "        self.X2_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X2_dist = ConditionalTransformedDistribution(self.X2_base_dist, self.X2_flow_transforms).condition(context_X2)\n",
    "        self.X2 = pyro.sample('X2', self.X2_dist)\n",
    "        #X1\n",
    "        context_X1=torch.cat([self.X5,self.X6],-1)\n",
    "        self.X1_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X1_dist = ConditionalTransformedDistribution(self.X1_base_dist, self.X1_flow_transforms).condition(context_X1)\n",
    "        self.X1 = pyro.sample('X1', self.X1_dist)\n",
    "        # Y\n",
    "        context_Y=torch.cat([self.X3,self.X2,self.X1],-1)\n",
    "        self.Y_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.Y_dist = ConditionalTransformedDistribution(self.Y_base_dist, self.Y_flow_transforms).condition(context_Y)\n",
    "        self.Y = pyro.sample('Y', self.Y_dist)\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X6, X5, X4,X3,X2,X1,Y ):\n",
    "        self.pgm_model()\n",
    "        Y_logp = self.Y_dist.log_prob(Y)\n",
    "        X1_logp = self.X1_dist.log_prob(X1)\n",
    "        X2_logp = self.X2_dist.log_prob(X2)\n",
    "        \n",
    "        X3_logp = torch.zeros(Y_logp.shape)                                  #self.X3_dist.log_prob(X3)                  \n",
    "        X4_logp = torch.zeros(Y_logp.shape)                                  #self.X4_dist.log_prob(X4)                                \n",
    "        X5_logp = torch.zeros(Y_logp.shape)                                  #self.X2_dist.log_prob(X2)\n",
    "        X6_logp = torch.zeros(Y_logp.shape)                                  #self.X1_dist.log_prob(X1)\n",
    "        \n",
    "        return {'X_6': X6_logp,'X_5': X5_logp,'X_4': X4_logp, 'X_3': X3_logp, 'X_2': X2_logp, 'X_1': X1_logp,'Y': Y_logp}\n",
    "\n",
    "    def clear(self):\n",
    "        self.X2_dist.clear_cache()\n",
    "        self.X1_dist.clear_cache()\n",
    "        self.Y_dist.clear_cache()\n",
    "        \n",
    "    def model(self):\n",
    "        self.pgm_model()\n",
    "        return self.X6, self.X5, self.X4, self.X3, self.X2, self.X1, self.Y\n",
    "\n",
    "    def scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam()\n",
    "            else:\n",
    "                return None\n",
    "        return pyro.poutine.reparam(self.model, config=config)(*args, **kwargs)\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.model()\n",
    "        return (*samples,)\n",
    "\n",
    "    def sample_scm(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.scm()\n",
    "        return (*samples,)\n",
    "    \n",
    "    def infer_exogeneous(self, **obs):\n",
    "        cond_sample = pyro.condition(self.sample, data=obs)\n",
    "        cond_trace = pyro.poutine.trace(cond_sample).get_trace(obs['X1'].shape[0])\n",
    "        output = {}\n",
    "        for name, node in cond_trace.nodes.items():\n",
    "            if 'fn' not in node.keys():\n",
    "                continue\n",
    "            fn = node['fn']\n",
    "            if isinstance(fn, Independent):\n",
    "                fn = fn.base_dist\n",
    "\n",
    "            if isinstance(fn, TransformedDistribution):\n",
    "                output[name + '_base'] = ComposeTransform(fn.transforms).inv(node['value'])\n",
    "        return output\n",
    "\n",
    "    def counterfactual(self, obs, intervention, num_particles=1):\n",
    "        counterfactuals = []\n",
    "        for _ in range(num_particles):\n",
    "            exogeneous = self.infer_exogeneous(**obs)\n",
    "            condition= {intervention.split('=')[0].split('(')[-1] :torch.full_like( obs['X4'],float(intervention.split('=')[-1][:-1]))}\n",
    "            condition['X3']=obs['X3']\n",
    "            condition['X4']=obs['X4']\n",
    "            condition['X6']=obs['X6']\n",
    "            #print('condition',condition)\n",
    "            counter = pyro.poutine.do(pyro.poutine.condition(self.sample_scm, data=exogeneous), data=condition)(obs['X1'].shape[0])\n",
    "            counterfactuals += [counter]\n",
    "        return {k: v for k, v in zip(('X_6','X_5','X_4', 'X_3', 'X_2', 'X_1','Y'), (torch.stack(c).mean(0) for c in zip(*counterfactuals)))}\n",
    "\n",
    "\n",
    "def conditionalscm_partial():\n",
    "    model = ConditionalSCM_partial(context_dim=2,normalize=True,spline_bins=8,spline_order='linear')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e3bae85-a5ab-461e-9545-1070e83c7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_partial=conditionalscm_partial()\n",
    "#print(scm_partial.sample(n_samples=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f35f4-73f8-43eb-8509-39789565ca92",
   "metadata": {},
   "source": [
    "Note that we only infer $\\epsilon_{2},\\epsilon_{1}$ and $\\epsilon_{Y}$. We hard intervene on $X_{5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349aaa71-eb88-4305-8bd6-7cdfa824e2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X2_base': tensor([[0.7083]], grad_fn=<IndexPutBackward0>), 'X1_base': tensor([[0.1800]], grad_fn=<IndexPutBackward0>), 'Y_base': tensor([[0.1564]], grad_fn=<IndexPutBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(scm_partial.infer_exogeneous(**data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1164abb-94cf-4a77-bbad-4e2b8b0cc444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X_6': tensor([[0.3960]]), 'X_5': tensor([[3.]]), 'X_4': tensor([[0.2559]]), 'X_3': tensor([[0.3896]]), 'X_2': tensor([[-0.0137]], grad_fn=<MeanBackward1>), 'X_1': tensor([[0.0103]], grad_fn=<MeanBackward1>), 'Y': tensor([[0.1713]], grad_fn=<MeanBackward1>)}\n"
     ]
    }
   ],
   "source": [
    "intervention='do(X5=3)'\n",
    "print(scm_partial.counterfactual(obs=data,intervention=intervention))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f6509-43a0-43d6-9ac1-34b658ee1052",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training function \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb588b-fc65-4f20-9c27-151830bc61aa",
   "metadata": {},
   "source": [
    "**training hyperparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad1124e7-37c3-42b1-8da3-4a87fbeec017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_seed=69    #default 69\n",
    "batch_size=64\n",
    "test_batch_size=32\n",
    "lr=3e-4\n",
    "weight_decay=1e-4\n",
    "epochs=100\n",
    "lr_annealing=False\n",
    "log_interval=100\n",
    "#seed=17        #default 17\n",
    "save='./logs'    #filepath to save training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51851a1e-813c-4584-9f21-303d009c1e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def milestone_step( optimizer, epoch):\n",
    "    if epoch in [epochs*0.5, epochs*0.75]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a241c90-11eb-4ffd-b5ef-5cc41d9302e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "from utils import mkdir\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filepath):\n",
    "    mkdir(filepath)\n",
    "    torch.save(state, os.path.join(filepath, 'flow_ckpt.pth.tar'))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(filepath, 'flow_ckpt.pth.tar'), os.path.join(filepath, 'flow_best.pth.tar'))\n",
    "        print('best is saved')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def train( model, optimizer, train_loader, epoch):\n",
    "    avg_loss = 0.\n",
    "    for batch_idx, (X6,X5,X4,X3, X2, X1,Y) in enumerate(train_loader):\n",
    "        X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "        optimizer.zero_grad()\n",
    "        log_p = model(X6, X5, X4, X3 ,X2, X1,Y)\n",
    "        loss = -torch.mean(log_p['X_6']+log_p['X_5']+log_p['X_4']+log_p['X_3']+log_p['X_2']+log_p['X_1'] +log_p['Y'])\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.clear()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\t| -LogProb X6: {:.6f}\\tX5: {:.6f}\\tX4: {:.6f}\\tX3: {:.6f} \\tX2: {:.6f}\\tX1: {:.6f} \\tY: {:.6f} \\tTotal: {:.6f}'.format(epoch, batch_idx * len(X1),\n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                -torch.mean(log_p['X_6']).item(), -torch.mean(log_p['X_5']).item(),\n",
    "                -torch.mean(log_p['X_4']).item(), -torch.mean(log_p['X_3']).item(),\n",
    "                -torch.mean(log_p['X_2']).item(), -torch.mean(log_p['X_1']).item(),-torch.mean(log_p['Y']).item(), -loss.item()))\n",
    "            \n",
    "            \n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss = 0.\n",
    "    for X6, X5, X4, X3, X2, X1,Y in test_loader:\n",
    "        with torch.no_grad():\n",
    "            X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "            log_p = model(X6, X5, X4, X3 ,X2, X1,Y)\n",
    "            test_loss += torch.mean(log_p['X_6']+log_p['X_5']+log_p['X_4']+log_p['X_3']+log_p['X_2']+log_p['X_1'] +log_p['Y'])\n",
    "                                    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average LogProb: {:.6f}\\n'.format(test_loss))\n",
    "    return test_loss\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "def main(model_name,filepath):\n",
    "    kwargs =  {}\n",
    "    dataset_train, dataset_test = get_features_dataset( dim=1) #random_seed=data_seed)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model = model_name\n",
    " \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 0)\n",
    "\n",
    "    best_loss = -200.\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        if not lr_annealing:\n",
    "            milestone_step(optimizer, epoch)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        train( model, optimizer, train_loader, epoch)\n",
    "        loss = test(model, test_loader)\n",
    "        \n",
    "        is_best = loss > best_loss\n",
    "        best_loss = max(loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, is_best, filepath=filepath)\n",
    "\n",
    "    del model\n",
    "    print('==> Best LogProb: {:.6f}, Time: {:.2f} min\\n'.format(best_loss, (time.time()-start_time)/60.))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f62ca9b9-2427-42cf-babc-441bc68b42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,path):\n",
    "    mkdir(path)\n",
    "    path = os.path.join(path, str(model.__class__.__name__))\n",
    "    mkdir(path)\n",
    "    filepath=path\n",
    "    main(model,filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f7217-49b7-4de7-b834-e829ebfcd612",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train scm \n",
    "---\n",
    "We haven't trained the models yet. We need to train them to get proper counterfactuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9da5cde9-f9c3-4592-9432-ab325777ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "Train Epoch: 0 [0/800 (0.0%)]\t| -LogProb X6: 3.768853\tX5: 12.846231\tX4: 6.484975\tX3: 34.736317 \tX2: 12.800082\tX1: 11.892523 \tY: 238.145935 \tTotal: -320.674927\n",
      "\n",
      "Test set: Average LogProb: -12.307648\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 1 [0/800 (0.0%)]\t| -LogProb X6: 3.859464\tX5: 12.834292\tX4: 7.218468\tX3: 38.581917 \tX2: 12.872317\tX1: 11.183006 \tY: 283.050323 \tTotal: -369.599792\n",
      "\n",
      "Test set: Average LogProb: -12.303592\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 2 [0/800 (0.0%)]\t| -LogProb X6: 3.776012\tX5: 13.281839\tX4: 7.088233\tX3: 36.312439 \tX2: 13.723106\tX1: 13.924568 \tY: 266.715240 \tTotal: -354.821472\n",
      "\n",
      "Test set: Average LogProb: -12.301545\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 3 [0/800 (0.0%)]\t| -LogProb X6: 3.585760\tX5: 13.993474\tX4: 6.295310\tX3: 30.765488 \tX2: 14.013019\tX1: 12.553345 \tY: 218.173798 \tTotal: -299.380188\n",
      "\n",
      "Test set: Average LogProb: -12.298140\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 4 [0/800 (0.0%)]\t| -LogProb X6: 4.089394\tX5: 16.732281\tX4: 5.882185\tX3: 41.013260 \tX2: 17.480335\tX1: 14.444036 \tY: 267.758118 \tTotal: -367.399628\n",
      "\n",
      "Test set: Average LogProb: -12.294168\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 5 [0/800 (0.0%)]\t| -LogProb X6: 3.381332\tX5: 12.029050\tX4: 7.185742\tX3: 40.983452 \tX2: 13.250429\tX1: 11.782094 \tY: 291.186523 \tTotal: -379.798645\n",
      "\n",
      "Test set: Average LogProb: -12.292171\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 6 [0/800 (0.0%)]\t| -LogProb X6: 3.894088\tX5: 13.110693\tX4: 6.141624\tX3: 46.557365 \tX2: 13.154716\tX1: 12.865703 \tY: 321.417542 \tTotal: -417.141785\n",
      "\n",
      "Test set: Average LogProb: -12.290494\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 7 [0/800 (0.0%)]\t| -LogProb X6: 3.814265\tX5: 15.897631\tX4: 6.912130\tX3: 41.439629 \tX2: 16.006441\tX1: 12.992618 \tY: 297.331512 \tTotal: -394.394226\n",
      "\n",
      "Test set: Average LogProb: -12.288723\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 8 [0/800 (0.0%)]\t| -LogProb X6: 4.070649\tX5: 16.659002\tX4: 6.086972\tX3: 41.412365 \tX2: 17.563644\tX1: 16.684443 \tY: 263.874359 \tTotal: -366.351440\n",
      "\n",
      "Test set: Average LogProb: -12.285439\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 9 [0/800 (0.0%)]\t| -LogProb X6: 3.834117\tX5: 15.586040\tX4: 6.908720\tX3: 32.293270 \tX2: 16.895844\tX1: 13.547882 \tY: 234.222260 \tTotal: -323.288147\n",
      "\n",
      "Test set: Average LogProb: -12.283215\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 10 [0/800 (0.0%)]\t| -LogProb X6: 3.650075\tX5: 12.998301\tX4: 6.556255\tX3: 49.221535 \tX2: 13.894131\tX1: 10.822380 \tY: 327.718750 \tTotal: -424.861389\n",
      "\n",
      "Test set: Average LogProb: -12.281144\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 11 [0/800 (0.0%)]\t| -LogProb X6: 3.890271\tX5: 16.533554\tX4: 5.747931\tX3: 44.464066 \tX2: 16.981684\tX1: 14.765197 \tY: 314.079163 \tTotal: -416.461853\n",
      "\n",
      "Test set: Average LogProb: -12.276510\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 12 [0/800 (0.0%)]\t| -LogProb X6: 3.191243\tX5: 9.371961\tX4: 6.071216\tX3: 34.596748 \tX2: 10.088513\tX1: 9.618968 \tY: 261.904572 \tTotal: -334.843231\n",
      "\n",
      "Test set: Average LogProb: -12.277166\n",
      "\n",
      "Train Epoch: 13 [0/800 (0.0%)]\t| -LogProb X6: 3.232639\tX5: 11.238473\tX4: 6.925994\tX3: 28.869480 \tX2: 12.033263\tX1: 10.104598 \tY: 216.421600 \tTotal: -288.826050\n",
      "\n",
      "Test set: Average LogProb: -12.274189\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 14 [0/800 (0.0%)]\t| -LogProb X6: 3.674428\tX5: 14.135163\tX4: 6.594284\tX3: 43.241596 \tX2: 14.463625\tX1: 13.391418 \tY: 300.618347 \tTotal: -396.118896\n",
      "\n",
      "Test set: Average LogProb: -12.271811\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 15 [0/800 (0.0%)]\t| -LogProb X6: 3.258677\tX5: 12.745420\tX4: 6.542526\tX3: 37.805946 \tX2: 12.853360\tX1: 12.334124 \tY: 241.822327 \tTotal: -327.362366\n",
      "\n",
      "Test set: Average LogProb: -12.269486\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 16 [0/800 (0.0%)]\t| -LogProb X6: 3.451611\tX5: 12.934924\tX4: 7.606396\tX3: 47.965343 \tX2: 13.664127\tX1: 9.253537 \tY: 358.398956 \tTotal: -453.274902\n",
      "\n",
      "Test set: Average LogProb: -12.267064\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 17 [0/800 (0.0%)]\t| -LogProb X6: 3.091496\tX5: 12.345798\tX4: 6.302794\tX3: 40.743530 \tX2: 11.739501\tX1: 9.638281 \tY: 298.983673 \tTotal: -382.845062\n",
      "\n",
      "Test set: Average LogProb: -12.263908\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 18 [0/800 (0.0%)]\t| -LogProb X6: 3.520866\tX5: 15.911164\tX4: 6.748013\tX3: 38.020435 \tX2: 16.795731\tX1: 12.252888 \tY: 252.641586 \tTotal: -345.890686\n",
      "\n",
      "Test set: Average LogProb: -12.262298\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 19 [0/800 (0.0%)]\t| -LogProb X6: 3.484570\tX5: 14.825556\tX4: 6.694081\tX3: 49.272594 \tX2: 16.484192\tX1: 14.295648 \tY: 338.857483 \tTotal: -443.914124\n",
      "\n",
      "Test set: Average LogProb: -12.260270\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 20 [0/800 (0.0%)]\t| -LogProb X6: 3.237717\tX5: 13.488560\tX4: 6.533466\tX3: 46.028351 \tX2: 14.539448\tX1: 11.303229 \tY: 343.701447 \tTotal: -438.832184\n",
      "\n",
      "Test set: Average LogProb: -12.256152\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 21 [0/800 (0.0%)]\t| -LogProb X6: 3.572352\tX5: 15.911124\tX4: 6.833261\tX3: 48.850391 \tX2: 16.766846\tX1: 13.160456 \tY: 376.494629 \tTotal: -481.589111\n",
      "\n",
      "Test set: Average LogProb: -12.255572\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 22 [0/800 (0.0%)]\t| -LogProb X6: 3.336718\tX5: 14.422027\tX4: 6.337163\tX3: 29.721874 \tX2: 14.220556\tX1: 12.528782 \tY: 193.916016 \tTotal: -274.483154\n",
      "\n",
      "Test set: Average LogProb: -12.253242\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 23 [0/800 (0.0%)]\t| -LogProb X6: 3.363350\tX5: 13.947960\tX4: 6.806740\tX3: 38.515560 \tX2: 13.484049\tX1: 11.579461 \tY: 274.278625 \tTotal: -361.975739\n",
      "\n",
      "Test set: Average LogProb: -12.250710\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 24 [0/800 (0.0%)]\t| -LogProb X6: 3.566047\tX5: 15.820492\tX4: 5.867253\tX3: 34.907162 \tX2: 16.510462\tX1: 15.977221 \tY: 240.554626 \tTotal: -333.203247\n",
      "\n",
      "Test set: Average LogProb: -12.248801\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 25 [0/800 (0.0%)]\t| -LogProb X6: 3.253566\tX5: 14.363655\tX4: 5.772815\tX3: 38.308659 \tX2: 15.942142\tX1: 8.908266 \tY: 274.298126 \tTotal: -360.847229\n",
      "\n",
      "Test set: Average LogProb: -12.246410\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 26 [0/800 (0.0%)]\t| -LogProb X6: 3.505757\tX5: 16.263920\tX4: 6.209918\tX3: 40.802925 \tX2: 17.693758\tX1: 15.128076 \tY: 260.077881 \tTotal: -359.682251\n",
      "\n",
      "Test set: Average LogProb: -12.244843\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 27 [0/800 (0.0%)]\t| -LogProb X6: 3.249341\tX5: 14.486410\tX4: 7.104155\tX3: 40.382984 \tX2: 16.100651\tX1: 11.527131 \tY: 271.141144 \tTotal: -363.991821\n",
      "\n",
      "Test set: Average LogProb: -12.243455\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 28 [0/800 (0.0%)]\t| -LogProb X6: 3.261856\tX5: 14.751633\tX4: 5.798244\tX3: 38.476086 \tX2: 14.730799\tX1: 11.724208 \tY: 250.327057 \tTotal: -339.069885\n",
      "\n",
      "Test set: Average LogProb: -12.240749\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 29 [0/800 (0.0%)]\t| -LogProb X6: 3.358907\tX5: 17.011021\tX4: 6.071008\tX3: 38.172630 \tX2: 17.995529\tX1: 14.245676 \tY: 257.651886 \tTotal: -354.506653\n",
      "\n",
      "Test set: Average LogProb: -12.239743\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 30 [0/800 (0.0%)]\t| -LogProb X6: 3.182309\tX5: 12.938644\tX4: 5.321590\tX3: 33.908878 \tX2: 14.199870\tX1: 8.612367 \tY: 249.286987 \tTotal: -327.450623\n",
      "\n",
      "Test set: Average LogProb: -12.237013\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 31 [0/800 (0.0%)]\t| -LogProb X6: 3.210763\tX5: 14.361996\tX4: 6.314853\tX3: 38.550884 \tX2: 14.026009\tX1: 13.519850 \tY: 290.855042 \tTotal: -380.839355\n",
      "\n",
      "Test set: Average LogProb: -12.236622\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 32 [0/800 (0.0%)]\t| -LogProb X6: 3.231788\tX5: 14.565908\tX4: 6.460760\tX3: 35.693924 \tX2: 14.367925\tX1: 11.982200 \tY: 257.987762 \tTotal: -344.290253\n",
      "\n",
      "Test set: Average LogProb: -12.234491\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 33 [0/800 (0.0%)]\t| -LogProb X6: 3.390406\tX5: 17.945953\tX4: 6.563884\tX3: 43.750633 \tX2: 18.654835\tX1: 13.581901 \tY: 338.310394 \tTotal: -442.198029\n",
      "\n",
      "Test set: Average LogProb: -12.232470\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 34 [0/800 (0.0%)]\t| -LogProb X6: 3.047261\tX5: 14.583235\tX4: 6.303779\tX3: 42.304752 \tX2: 15.134426\tX1: 14.916563 \tY: 293.796356 \tTotal: -390.086365\n",
      "\n",
      "Test set: Average LogProb: -12.232640\n",
      "\n",
      "Train Epoch: 35 [0/800 (0.0%)]\t| -LogProb X6: 3.152490\tX5: 14.663968\tX4: 6.932453\tX3: 37.676300 \tX2: 15.363331\tX1: 12.732184 \tY: 248.404068 \tTotal: -338.924805\n",
      "\n",
      "Test set: Average LogProb: -12.231358\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 36 [0/800 (0.0%)]\t| -LogProb X6: 2.908076\tX5: 12.440791\tX4: 6.276284\tX3: 30.537384 \tX2: 14.145666\tX1: 13.140538 \tY: 233.995483 \tTotal: -313.444214\n",
      "\n",
      "Test set: Average LogProb: -12.229140\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 37 [0/800 (0.0%)]\t| -LogProb X6: 2.764482\tX5: 13.121962\tX4: 6.631409\tX3: 35.467167 \tX2: 12.908035\tX1: 12.675449 \tY: 268.459503 \tTotal: -352.027985\n",
      "\n",
      "Test set: Average LogProb: -12.226211\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 38 [0/800 (0.0%)]\t| -LogProb X6: 3.153182\tX5: 16.747847\tX4: 6.807386\tX3: 50.421638 \tX2: 17.093693\tX1: 14.739352 \tY: 321.106293 \tTotal: -430.069336\n",
      "\n",
      "Test set: Average LogProb: -12.225138\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 39 [0/800 (0.0%)]\t| -LogProb X6: 2.886180\tX5: 12.828411\tX4: 6.187301\tX3: 39.306923 \tX2: 11.940800\tX1: 11.542801 \tY: 291.187225 \tTotal: -375.879608\n",
      "\n",
      "Test set: Average LogProb: -12.224797\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 40 [0/800 (0.0%)]\t| -LogProb X6: 2.378981\tX5: 10.004475\tX4: 6.799087\tX3: 44.400837 \tX2: 10.784992\tX1: 9.919935 \tY: 325.769867 \tTotal: -410.058167\n",
      "\n",
      "Test set: Average LogProb: -12.221929\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 41 [0/800 (0.0%)]\t| -LogProb X6: 2.942599\tX5: 13.693113\tX4: 5.825377\tX3: 42.234489 \tX2: 14.351155\tX1: 10.903870 \tY: 290.152832 \tTotal: -380.103424\n",
      "\n",
      "Test set: Average LogProb: -12.221445\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 42 [0/800 (0.0%)]\t| -LogProb X6: 2.715429\tX5: 12.616941\tX4: 6.574047\tX3: 44.546490 \tX2: 13.384056\tX1: 13.165724 \tY: 326.472321 \tTotal: -419.475037\n",
      "\n",
      "Test set: Average LogProb: -12.223483\n",
      "\n",
      "Train Epoch: 43 [0/800 (0.0%)]\t| -LogProb X6: 2.722755\tX5: 12.577238\tX4: 5.942948\tX3: 40.198338 \tX2: 12.944817\tX1: 12.021952 \tY: 300.362061 \tTotal: -386.770081\n",
      "\n",
      "Test set: Average LogProb: -12.219541\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 44 [0/800 (0.0%)]\t| -LogProb X6: 2.931811\tX5: 16.078726\tX4: 6.193550\tX3: 41.993870 \tX2: 17.881285\tX1: 11.962886 \tY: 263.418579 \tTotal: -360.460724\n",
      "\n",
      "Test set: Average LogProb: -12.219032\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 45 [0/800 (0.0%)]\t| -LogProb X6: 2.874731\tX5: 14.451715\tX4: 5.809649\tX3: 28.261959 \tX2: 15.480131\tX1: 11.921322 \tY: 191.125595 \tTotal: -269.925110\n",
      "\n",
      "Test set: Average LogProb: -12.218280\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 46 [0/800 (0.0%)]\t| -LogProb X6: 2.646910\tX5: 12.089428\tX4: 6.144794\tX3: 38.318859 \tX2: 12.706333\tX1: 12.969816 \tY: 280.518188 \tTotal: -365.394348\n",
      "\n",
      "Test set: Average LogProb: -12.216455\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 47 [0/800 (0.0%)]\t| -LogProb X6: 2.814346\tX5: 14.086733\tX4: 6.456354\tX3: 41.544579 \tX2: 15.100738\tX1: 12.655966 \tY: 299.499023 \tTotal: -392.157745\n",
      "\n",
      "Test set: Average LogProb: -12.218179\n",
      "\n",
      "Train Epoch: 48 [0/800 (0.0%)]\t| -LogProb X6: 2.770107\tX5: 14.691317\tX4: 6.513621\tX3: 43.959988 \tX2: 14.755645\tX1: 16.086676 \tY: 311.397949 \tTotal: -410.175354\n",
      "\n",
      "Test set: Average LogProb: -12.213143\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 49 [0/800 (0.0%)]\t| -LogProb X6: 2.716042\tX5: 13.288356\tX4: 6.004160\tX3: 32.032158 \tX2: 13.815571\tX1: 12.382812 \tY: 228.046417 \tTotal: -308.285492\n",
      "\n",
      "Test set: Average LogProb: -12.213284\n",
      "\n",
      "Train Epoch: 50 [0/800 (0.0%)]\t| -LogProb X6: 2.425034\tX5: 9.576607\tX4: 6.059495\tX3: 43.798328 \tX2: 10.131540\tX1: 10.498879 \tY: 330.821289 \tTotal: -413.311157\n",
      "\n",
      "Test set: Average LogProb: -12.213575\n",
      "\n",
      "Train Epoch: 51 [0/800 (0.0%)]\t| -LogProb X6: 2.840726\tX5: 14.841089\tX4: 6.146598\tX3: 37.846802 \tX2: 15.617714\tX1: 14.255384 \tY: 263.257050 \tTotal: -354.805328\n",
      "\n",
      "Test set: Average LogProb: -12.213690\n",
      "\n",
      "Train Epoch: 52 [0/800 (0.0%)]\t| -LogProb X6: 2.572631\tX5: 11.906612\tX4: 5.231833\tX3: 36.554760 \tX2: 12.369318\tX1: 11.218572 \tY: 259.779510 \tTotal: -339.633240\n",
      "\n",
      "Test set: Average LogProb: -12.213618\n",
      "\n",
      "Train Epoch: 53 [0/800 (0.0%)]\t| -LogProb X6: 2.778055\tX5: 12.858067\tX4: 5.550748\tX3: 36.636604 \tX2: 14.347464\tX1: 12.568506 \tY: 243.694305 \tTotal: -328.433716\n",
      "\n",
      "Test set: Average LogProb: -12.212646\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 54 [0/800 (0.0%)]\t| -LogProb X6: 2.644532\tX5: 12.540792\tX4: 5.802898\tX3: 41.868088 \tX2: 13.185321\tX1: 10.455368 \tY: 305.119965 \tTotal: -391.616943\n",
      "\n",
      "Test set: Average LogProb: -12.211796\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 55 [0/800 (0.0%)]\t| -LogProb X6: 2.895103\tX5: 17.072893\tX4: 5.904294\tX3: 36.823071 \tX2: 17.429657\tX1: 13.888281 \tY: 235.568848 \tTotal: -329.582184\n",
      "\n",
      "Test set: Average LogProb: -12.212475\n",
      "\n",
      "Train Epoch: 56 [0/800 (0.0%)]\t| -LogProb X6: 2.818088\tX5: 15.524027\tX4: 6.262150\tX3: 33.318901 \tX2: 16.443253\tX1: 12.537454 \tY: 248.269562 \tTotal: -335.173431\n",
      "\n",
      "Test set: Average LogProb: -12.213733\n",
      "\n",
      "Train Epoch: 57 [0/800 (0.0%)]\t| -LogProb X6: 2.630250\tX5: 13.532427\tX4: 6.206335\tX3: 40.324516 \tX2: 13.809204\tX1: 10.364387 \tY: 271.967316 \tTotal: -358.834442\n",
      "\n",
      "Test set: Average LogProb: -12.213445\n",
      "\n",
      "Train Epoch: 58 [0/800 (0.0%)]\t| -LogProb X6: 2.759496\tX5: 15.226910\tX4: 6.276645\tX3: 44.728569 \tX2: 15.076722\tX1: 14.934669 \tY: 326.451752 \tTotal: -425.454773\n",
      "\n",
      "Test set: Average LogProb: -12.212766\n",
      "\n",
      "Train Epoch: 59 [0/800 (0.0%)]\t| -LogProb X6: 2.682444\tX5: 13.772131\tX4: 6.141088\tX3: 48.404858 \tX2: 13.655919\tX1: 12.092892 \tY: 362.366852 \tTotal: -459.116180\n",
      "\n",
      "Test set: Average LogProb: -12.211849\n",
      "\n",
      "Train Epoch: 60 [0/800 (0.0%)]\t| -LogProb X6: 2.393456\tX5: 12.045095\tX4: 5.964743\tX3: 31.970503 \tX2: 12.063116\tX1: 11.204823 \tY: 234.868683 \tTotal: -310.510406\n",
      "\n",
      "Test set: Average LogProb: -12.211664\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 61 [0/800 (0.0%)]\t| -LogProb X6: 2.857132\tX5: 15.229198\tX4: 6.317772\tX3: 31.767546 \tX2: 15.525187\tX1: 12.570775 \tY: 219.600220 \tTotal: -303.867828\n",
      "\n",
      "Test set: Average LogProb: -12.211757\n",
      "\n",
      "Train Epoch: 62 [0/800 (0.0%)]\t| -LogProb X6: 2.619203\tX5: 12.220753\tX4: 6.664385\tX3: 38.071514 \tX2: 13.082642\tX1: 11.756056 \tY: 280.557434 \tTotal: -364.971985\n",
      "\n",
      "Test set: Average LogProb: -12.210950\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 63 [0/800 (0.0%)]\t| -LogProb X6: 2.695141\tX5: 12.309175\tX4: 6.269057\tX3: 41.163017 \tX2: 13.821894\tX1: 13.263752 \tY: 285.527252 \tTotal: -375.049225\n",
      "\n",
      "Test set: Average LogProb: -12.211267\n",
      "\n",
      "Train Epoch: 64 [0/800 (0.0%)]\t| -LogProb X6: 2.738139\tX5: 13.308590\tX4: 5.954619\tX3: 38.185432 \tX2: 13.844375\tX1: 10.334931 \tY: 277.439606 \tTotal: -361.805695\n",
      "\n",
      "Test set: Average LogProb: -12.212482\n",
      "\n",
      "Train Epoch: 65 [0/800 (0.0%)]\t| -LogProb X6: 2.417110\tX5: 10.975534\tX4: 5.755004\tX3: 42.779636 \tX2: 12.674980\tX1: 9.962529 \tY: 311.546570 \tTotal: -396.111359\n",
      "\n",
      "Test set: Average LogProb: -12.211825\n",
      "\n",
      "Train Epoch: 66 [0/800 (0.0%)]\t| -LogProb X6: 2.546496\tX5: 12.103313\tX4: 6.186092\tX3: 47.382370 \tX2: 11.858477\tX1: 9.756257 \tY: 327.340210 \tTotal: -417.173248\n",
      "\n",
      "Test set: Average LogProb: -12.210599\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 67 [0/800 (0.0%)]\t| -LogProb X6: 2.329405\tX5: 10.232342\tX4: 6.242864\tX3: 36.070435 \tX2: 10.028385\tX1: 11.125776 \tY: 262.770844 \tTotal: -338.800049\n",
      "\n",
      "Test set: Average LogProb: -12.212129\n",
      "\n",
      "Train Epoch: 68 [0/800 (0.0%)]\t| -LogProb X6: 2.691428\tX5: 13.986402\tX4: 5.311378\tX3: 36.593964 \tX2: 13.945452\tX1: 12.451457 \tY: 242.580383 \tTotal: -327.560455\n",
      "\n",
      "Test set: Average LogProb: -12.210623\n",
      "\n",
      "Train Epoch: 69 [0/800 (0.0%)]\t| -LogProb X6: 2.655264\tX5: 12.567123\tX4: 6.421972\tX3: 41.242676 \tX2: 12.661092\tX1: 13.275472 \tY: 304.800201 \tTotal: -393.623779\n",
      "\n",
      "Test set: Average LogProb: -12.212127\n",
      "\n",
      "Train Epoch: 70 [0/800 (0.0%)]\t| -LogProb X6: 2.564446\tX5: 12.771705\tX4: 5.877553\tX3: 34.358883 \tX2: 12.673660\tX1: 8.817835 \tY: 226.267609 \tTotal: -303.331696\n",
      "\n",
      "Test set: Average LogProb: -12.212652\n",
      "\n",
      "Train Epoch: 71 [0/800 (0.0%)]\t| -LogProb X6: 2.795080\tX5: 15.507325\tX4: 6.262937\tX3: 39.796017 \tX2: 15.786284\tX1: 12.072239 \tY: 258.896973 \tTotal: -351.116852\n",
      "\n",
      "Test set: Average LogProb: -12.209847\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 72 [0/800 (0.0%)]\t| -LogProb X6: 2.840548\tX5: 15.412357\tX4: 5.823065\tX3: 51.297573 \tX2: 15.325550\tX1: 14.536259 \tY: 363.303772 \tTotal: -468.539093\n",
      "\n",
      "Test set: Average LogProb: -12.211010\n",
      "\n",
      "Train Epoch: 73 [0/800 (0.0%)]\t| -LogProb X6: 2.605756\tX5: 13.157255\tX4: 5.885282\tX3: 37.975647 \tX2: 13.739611\tX1: 10.597874 \tY: 254.793610 \tTotal: -338.755005\n",
      "\n",
      "Test set: Average LogProb: -12.210636\n",
      "\n",
      "Train Epoch: 74 [0/800 (0.0%)]\t| -LogProb X6: 3.210063\tX5: 19.548529\tX4: 5.812748\tX3: 32.569695 \tX2: 19.323097\tX1: 14.897642 \tY: 226.052032 \tTotal: -321.413818\n",
      "\n",
      "Test set: Average LogProb: -12.210040\n",
      "\n",
      "Train Epoch: 75 [0/800 (0.0%)]\t| -LogProb X6: 2.496155\tX5: 12.895088\tX4: 5.785315\tX3: 30.326286 \tX2: 13.801667\tX1: 12.402621 \tY: 221.462204 \tTotal: -299.169342\n",
      "\n",
      "Test set: Average LogProb: -12.210275\n",
      "\n",
      "Train Epoch: 76 [0/800 (0.0%)]\t| -LogProb X6: 2.664502\tX5: 13.738004\tX4: 5.841279\tX3: 41.782681 \tX2: 14.450583\tX1: 15.550529 \tY: 326.138062 \tTotal: -420.165649\n",
      "\n",
      "Test set: Average LogProb: -12.209922\n",
      "\n",
      "Train Epoch: 77 [0/800 (0.0%)]\t| -LogProb X6: 2.498597\tX5: 12.923222\tX4: 5.969497\tX3: 37.677475 \tX2: 12.485922\tX1: 11.353443 \tY: 260.302399 \tTotal: -343.210571\n",
      "\n",
      "Test set: Average LogProb: -12.210659\n",
      "\n",
      "Train Epoch: 78 [0/800 (0.0%)]\t| -LogProb X6: 2.675413\tX5: 13.084971\tX4: 5.761931\tX3: 41.461815 \tX2: 13.243353\tX1: 11.278778 \tY: 271.335327 \tTotal: -358.841583\n",
      "\n",
      "Test set: Average LogProb: -12.210558\n",
      "\n",
      "Train Epoch: 79 [0/800 (0.0%)]\t| -LogProb X6: 2.683988\tX5: 12.351628\tX4: 5.914241\tX3: 41.853081 \tX2: 12.317408\tX1: 10.468725 \tY: 300.546509 \tTotal: -386.135590\n",
      "\n",
      "Test set: Average LogProb: -12.210801\n",
      "\n",
      "Train Epoch: 80 [0/800 (0.0%)]\t| -LogProb X6: 2.571410\tX5: 13.683432\tX4: 5.356238\tX3: 33.592915 \tX2: 13.651476\tX1: 12.355992 \tY: 236.902527 \tTotal: -318.113983\n",
      "\n",
      "Test set: Average LogProb: -12.210895\n",
      "\n",
      "Train Epoch: 81 [0/800 (0.0%)]\t| -LogProb X6: 2.575004\tX5: 12.493313\tX4: 5.549268\tX3: 39.096878 \tX2: 12.981701\tX1: 11.111791 \tY: 272.564301 \tTotal: -356.372223\n",
      "\n",
      "Test set: Average LogProb: -12.209765\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 82 [0/800 (0.0%)]\t| -LogProb X6: 2.980186\tX5: 15.606606\tX4: 6.012344\tX3: 28.392321 \tX2: 15.500830\tX1: 14.704792 \tY: 224.810303 \tTotal: -308.007355\n",
      "\n",
      "Test set: Average LogProb: -12.210789\n",
      "\n",
      "Train Epoch: 83 [0/800 (0.0%)]\t| -LogProb X6: 2.859029\tX5: 14.549633\tX4: 5.806556\tX3: 33.761963 \tX2: 16.246359\tX1: 14.182183 \tY: 231.406830 \tTotal: -318.812561\n",
      "\n",
      "Test set: Average LogProb: -12.210296\n",
      "\n",
      "Train Epoch: 84 [0/800 (0.0%)]\t| -LogProb X6: 2.751949\tX5: 15.670650\tX4: 5.952852\tX3: 35.529438 \tX2: 15.641930\tX1: 12.230878 \tY: 237.224991 \tTotal: -325.002686\n",
      "\n",
      "Test set: Average LogProb: -12.211667\n",
      "\n",
      "Train Epoch: 85 [0/800 (0.0%)]\t| -LogProb X6: 2.484578\tX5: 14.329548\tX4: 6.634953\tX3: 42.106369 \tX2: 14.889466\tX1: 12.633018 \tY: 302.903931 \tTotal: -395.981873\n",
      "\n",
      "Test set: Average LogProb: -12.210723\n",
      "\n",
      "Train Epoch: 86 [0/800 (0.0%)]\t| -LogProb X6: 2.715637\tX5: 16.100878\tX4: 5.851784\tX3: 34.843590 \tX2: 16.662395\tX1: 12.681181 \tY: 250.061707 \tTotal: -338.917175\n",
      "\n",
      "Test set: Average LogProb: -12.210440\n",
      "\n",
      "Train Epoch: 87 [0/800 (0.0%)]\t| -LogProb X6: 2.820632\tX5: 15.481259\tX4: 6.359642\tX3: 39.048676 \tX2: 16.226568\tX1: 11.830215 \tY: 256.312317 \tTotal: -348.079285\n",
      "\n",
      "Test set: Average LogProb: -12.210640\n",
      "\n",
      "Train Epoch: 88 [0/800 (0.0%)]\t| -LogProb X6: 2.734251\tX5: 14.976577\tX4: 5.926610\tX3: 34.682159 \tX2: 16.198170\tX1: 14.446066 \tY: 231.456924 \tTotal: -320.420776\n",
      "\n",
      "Test set: Average LogProb: -12.211047\n",
      "\n",
      "Train Epoch: 89 [0/800 (0.0%)]\t| -LogProb X6: 2.602330\tX5: 13.466604\tX4: 6.218266\tX3: 39.863731 \tX2: 14.010475\tX1: 9.884387 \tY: 281.583649 \tTotal: -367.629425\n",
      "\n",
      "Test set: Average LogProb: -12.210874\n",
      "\n",
      "Train Epoch: 90 [0/800 (0.0%)]\t| -LogProb X6: 2.580768\tX5: 13.821472\tX4: 5.271396\tX3: 31.932613 \tX2: 14.248642\tX1: 13.142236 \tY: 208.227142 \tTotal: -289.224243\n",
      "\n",
      "Test set: Average LogProb: -12.210976\n",
      "\n",
      "Train Epoch: 91 [0/800 (0.0%)]\t| -LogProb X6: 2.560883\tX5: 12.664693\tX4: 6.420921\tX3: 38.819221 \tX2: 14.907784\tX1: 12.360394 \tY: 335.551178 \tTotal: -423.285065\n",
      "\n",
      "Test set: Average LogProb: -12.209858\n",
      "\n",
      "Train Epoch: 92 [0/800 (0.0%)]\t| -LogProb X6: 2.741555\tX5: 14.600703\tX4: 6.333127\tX3: 43.947487 \tX2: 14.875483\tX1: 12.374017 \tY: 313.502716 \tTotal: -408.375092\n",
      "\n",
      "Test set: Average LogProb: -12.209026\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 93 [0/800 (0.0%)]\t| -LogProb X6: 2.428043\tX5: 12.962478\tX4: 5.421448\tX3: 37.543091 \tX2: 12.791710\tX1: 11.954900 \tY: 259.779144 \tTotal: -342.880798\n",
      "\n",
      "Test set: Average LogProb: -12.210178\n",
      "\n",
      "Train Epoch: 94 [0/800 (0.0%)]\t| -LogProb X6: 2.755030\tX5: 12.498480\tX4: 6.020312\tX3: 36.740215 \tX2: 12.949249\tX1: 13.580117 \tY: 291.119843 \tTotal: -375.663239\n",
      "\n",
      "Test set: Average LogProb: -12.210767\n",
      "\n",
      "Train Epoch: 95 [0/800 (0.0%)]\t| -LogProb X6: 2.678259\tX5: 13.125662\tX4: 5.817146\tX3: 45.313141 \tX2: 12.936943\tX1: 12.987349 \tY: 334.419189 \tTotal: -427.277679\n",
      "\n",
      "Test set: Average LogProb: -12.211060\n",
      "\n",
      "Train Epoch: 96 [0/800 (0.0%)]\t| -LogProb X6: 2.371845\tX5: 11.041027\tX4: 6.221766\tX3: 43.808151 \tX2: 11.062978\tX1: 10.994154 \tY: 307.568604 \tTotal: -393.068512\n",
      "\n",
      "Test set: Average LogProb: -12.210160\n",
      "\n",
      "Train Epoch: 97 [0/800 (0.0%)]\t| -LogProb X6: 2.489930\tX5: 12.534113\tX4: 6.275156\tX3: 43.050320 \tX2: 13.266199\tX1: 13.867222 \tY: 317.240753 \tTotal: -408.723663\n",
      "\n",
      "Test set: Average LogProb: -12.209600\n",
      "\n",
      "Train Epoch: 98 [0/800 (0.0%)]\t| -LogProb X6: 2.731361\tX5: 14.811277\tX4: 5.478198\tX3: 39.018402 \tX2: 15.133936\tX1: 11.949303 \tY: 267.049957 \tTotal: -356.172424\n",
      "\n",
      "Test set: Average LogProb: -12.210260\n",
      "\n",
      "Train Epoch: 99 [0/800 (0.0%)]\t| -LogProb X6: 2.550101\tX5: 11.479220\tX4: 5.916162\tX3: 45.947052 \tX2: 12.778148\tX1: 10.955773 \tY: 362.072113 \tTotal: -451.698578\n",
      "\n",
      "Test set: Average LogProb: -12.209795\n",
      "\n",
      "==> Best LogProb: -12.209026, Time: 1.00 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(scm,save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3868fb-b71d-48a0-9f36-31da63c86419",
   "metadata": {
    "tags": []
   },
   "source": [
    "### train scm_partial\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aadff18-0b6c-4eed-be45-ed19dc4ac167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "Train Epoch: 0 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.280237\tX1: 9.489903 \tY: 310.747986 \tTotal: -333.518127\n",
      "\n",
      "Test set: Average LogProb: -10.139407\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 1 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.583876\tX1: 14.212538 \tY: 270.703461 \tTotal: -301.499878\n",
      "\n",
      "Test set: Average LogProb: -10.137863\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 2 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.237238\tX1: 11.704982 \tY: 214.793564 \tTotal: -240.735779\n",
      "\n",
      "Test set: Average LogProb: -10.136669\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 3 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.095730\tX1: 11.991588 \tY: 318.376801 \tTotal: -342.464111\n",
      "\n",
      "Test set: Average LogProb: -10.138023\n",
      "\n",
      "Train Epoch: 4 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.819696\tX1: 12.288887 \tY: 285.011353 \tTotal: -312.119934\n",
      "\n",
      "Test set: Average LogProb: -10.137185\n",
      "\n",
      "Train Epoch: 5 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.771206\tX1: 12.624113 \tY: 320.345581 \tTotal: -346.740906\n",
      "\n",
      "Test set: Average LogProb: -10.136518\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 6 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.273619\tX1: 10.391761 \tY: 300.263428 \tTotal: -322.928833\n",
      "\n",
      "Test set: Average LogProb: -10.136072\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 7 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.096210\tX1: 12.594806 \tY: 332.624542 \tTotal: -361.315613\n",
      "\n",
      "Test set: Average LogProb: -10.136610\n",
      "\n",
      "Train Epoch: 8 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.624039\tX1: 13.878032 \tY: 219.931015 \tTotal: -248.433105\n",
      "\n",
      "Test set: Average LogProb: -10.135909\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 9 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.029371\tX1: 13.427001 \tY: 216.588684 \tTotal: -246.045074\n",
      "\n",
      "Test set: Average LogProb: -10.135098\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 10 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.877005\tX1: 12.780323 \tY: 289.280090 \tTotal: -317.937439\n",
      "\n",
      "Test set: Average LogProb: -10.134486\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 11 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.793953\tX1: 13.276734 \tY: 245.158142 \tTotal: -273.228821\n",
      "\n",
      "Test set: Average LogProb: -10.134519\n",
      "\n",
      "Train Epoch: 12 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.983835\tX1: 12.606998 \tY: 288.429932 \tTotal: -317.020782\n",
      "\n",
      "Test set: Average LogProb: -10.134871\n",
      "\n",
      "Train Epoch: 13 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.295701\tX1: 11.998632 \tY: 258.815125 \tTotal: -286.109467\n",
      "\n",
      "Test set: Average LogProb: -10.134326\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 14 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.622581\tX1: 12.114916 \tY: 270.578796 \tTotal: -298.316284\n",
      "\n",
      "Test set: Average LogProb: -10.133665\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 15 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 17.233917\tX1: 14.105348 \tY: 326.076752 \tTotal: -357.416016\n",
      "\n",
      "Test set: Average LogProb: -10.132477\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 16 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.519437\tX1: 14.393208 \tY: 299.801666 \tTotal: -329.714325\n",
      "\n",
      "Test set: Average LogProb: -10.132648\n",
      "\n",
      "Train Epoch: 17 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.564477\tX1: 11.321358 \tY: 286.438660 \tTotal: -311.324493\n",
      "\n",
      "Test set: Average LogProb: -10.134248\n",
      "\n",
      "Train Epoch: 18 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.497516\tX1: 10.640363 \tY: 327.171997 \tTotal: -350.309906\n",
      "\n",
      "Test set: Average LogProb: -10.132330\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 19 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.746476\tX1: 14.919209 \tY: 284.538605 \tTotal: -313.204285\n",
      "\n",
      "Test set: Average LogProb: -10.132801\n",
      "\n",
      "Train Epoch: 20 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.856689\tX1: 13.357258 \tY: 329.158722 \tTotal: -357.372681\n",
      "\n",
      "Test set: Average LogProb: -10.131439\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 21 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.382809\tX1: 12.629790 \tY: 322.082245 \tTotal: -348.094849\n",
      "\n",
      "Test set: Average LogProb: -10.132669\n",
      "\n",
      "Train Epoch: 22 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.158410\tX1: 10.505226 \tY: 420.613556 \tTotal: -444.277313\n",
      "\n",
      "Test set: Average LogProb: -10.131850\n",
      "\n",
      "Train Epoch: 23 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.188375\tX1: 14.405532 \tY: 306.720612 \tTotal: -337.314545\n",
      "\n",
      "Test set: Average LogProb: -10.131863\n",
      "\n",
      "Train Epoch: 24 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.098051\tX1: 10.829176 \tY: 296.077423 \tTotal: -321.004669\n",
      "\n",
      "Test set: Average LogProb: -10.129497\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 25 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 11.837444\tX1: 10.565487 \tY: 298.601807 \tTotal: -321.004700\n",
      "\n",
      "Test set: Average LogProb: -10.131189\n",
      "\n",
      "Train Epoch: 26 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.833823\tX1: 10.888010 \tY: 242.924942 \tTotal: -268.646790\n",
      "\n",
      "Test set: Average LogProb: -10.130448\n",
      "\n",
      "Train Epoch: 27 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.187628\tX1: 12.922878 \tY: 305.225861 \tTotal: -331.336334\n",
      "\n",
      "Test set: Average LogProb: -10.131343\n",
      "\n",
      "Train Epoch: 28 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 19.452721\tX1: 12.456140 \tY: 262.133331 \tTotal: -294.042175\n",
      "\n",
      "Test set: Average LogProb: -10.130033\n",
      "\n",
      "Train Epoch: 29 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.720447\tX1: 10.226696 \tY: 330.464691 \tTotal: -353.411804\n",
      "\n",
      "Test set: Average LogProb: -10.130158\n",
      "\n",
      "Train Epoch: 30 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.207554\tX1: 13.318791 \tY: 279.693146 \tTotal: -305.219482\n",
      "\n",
      "Test set: Average LogProb: -10.131088\n",
      "\n",
      "Train Epoch: 31 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.245840\tX1: 11.487076 \tY: 216.534653 \tTotal: -243.267563\n",
      "\n",
      "Test set: Average LogProb: -10.130336\n",
      "\n",
      "Train Epoch: 32 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.297575\tX1: 9.530217 \tY: 263.111145 \tTotal: -287.938965\n",
      "\n",
      "Test set: Average LogProb: -10.130651\n",
      "\n",
      "Train Epoch: 33 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.967957\tX1: 12.172180 \tY: 324.025024 \tTotal: -349.165192\n",
      "\n",
      "Test set: Average LogProb: -10.130306\n",
      "\n",
      "Train Epoch: 34 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 11.827986\tX1: 9.109883 \tY: 288.550720 \tTotal: -309.488586\n",
      "\n",
      "Test set: Average LogProb: -10.130091\n",
      "\n",
      "Train Epoch: 35 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 17.176714\tX1: 14.768734 \tY: 217.589554 \tTotal: -249.535004\n",
      "\n",
      "Test set: Average LogProb: -10.129622\n",
      "\n",
      "Train Epoch: 36 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.598577\tX1: 13.038609 \tY: 241.725952 \tTotal: -268.363129\n",
      "\n",
      "Test set: Average LogProb: -10.129306\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 37 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.602766\tX1: 14.724954 \tY: 295.841827 \tTotal: -325.169556\n",
      "\n",
      "Test set: Average LogProb: -10.128922\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 38 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 17.563950\tX1: 13.360658 \tY: 234.515060 \tTotal: -265.439697\n",
      "\n",
      "Test set: Average LogProb: -10.128671\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 39 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.405897\tX1: 13.632530 \tY: 298.499908 \tTotal: -325.538330\n",
      "\n",
      "Test set: Average LogProb: -10.128593\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 40 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.130274\tX1: 11.046235 \tY: 294.886383 \tTotal: -318.062897\n",
      "\n",
      "Test set: Average LogProb: -10.129806\n",
      "\n",
      "Train Epoch: 41 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.941162\tX1: 11.278158 \tY: 362.391541 \tTotal: -388.610840\n",
      "\n",
      "Test set: Average LogProb: -10.129002\n",
      "\n",
      "Train Epoch: 42 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.515522\tX1: 11.437442 \tY: 225.332870 \tTotal: -250.285858\n",
      "\n",
      "Test set: Average LogProb: -10.128558\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 43 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.275257\tX1: 12.286866 \tY: 283.206940 \tTotal: -311.769043\n",
      "\n",
      "Test set: Average LogProb: -10.128753\n",
      "\n",
      "Train Epoch: 44 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.738832\tX1: 17.665001 \tY: 332.886475 \tTotal: -367.290283\n",
      "\n",
      "Test set: Average LogProb: -10.129073\n",
      "\n",
      "Train Epoch: 45 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.059307\tX1: 9.608533 \tY: 328.772186 \tTotal: -353.440033\n",
      "\n",
      "Test set: Average LogProb: -10.129279\n",
      "\n",
      "Train Epoch: 46 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.032161\tX1: 13.094280 \tY: 338.938110 \tTotal: -366.064545\n",
      "\n",
      "Test set: Average LogProb: -10.127915\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 47 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.509436\tX1: 12.115507 \tY: 252.053375 \tTotal: -278.678345\n",
      "\n",
      "Test set: Average LogProb: -10.128043\n",
      "\n",
      "Train Epoch: 48 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.467467\tX1: 14.725047 \tY: 367.405884 \tTotal: -397.598450\n",
      "\n",
      "Test set: Average LogProb: -10.127700\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 49 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.908902\tX1: 14.100322 \tY: 231.030869 \tTotal: -261.040070\n",
      "\n",
      "Test set: Average LogProb: -10.128188\n",
      "\n",
      "Train Epoch: 50 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.122488\tX1: 15.367800 \tY: 245.515045 \tTotal: -276.005341\n",
      "\n",
      "Test set: Average LogProb: -10.127949\n",
      "\n",
      "Train Epoch: 51 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.644339\tX1: 13.804965 \tY: 289.434692 \tTotal: -317.884003\n",
      "\n",
      "Test set: Average LogProb: -10.128836\n",
      "\n",
      "Train Epoch: 52 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.238127\tX1: 10.665051 \tY: 255.546341 \tTotal: -281.449524\n",
      "\n",
      "Test set: Average LogProb: -10.127720\n",
      "\n",
      "Train Epoch: 53 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.506119\tX1: 13.879131 \tY: 323.268463 \tTotal: -353.653687\n",
      "\n",
      "Test set: Average LogProb: -10.128154\n",
      "\n",
      "Train Epoch: 54 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 10.124436\tX1: 8.756083 \tY: 237.289978 \tTotal: -256.170471\n",
      "\n",
      "Test set: Average LogProb: -10.128331\n",
      "\n",
      "Train Epoch: 55 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 11.817529\tX1: 11.613193 \tY: 228.127136 \tTotal: -251.557861\n",
      "\n",
      "Test set: Average LogProb: -10.127039\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 56 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.358915\tX1: 11.059300 \tY: 334.661591 \tTotal: -358.079773\n",
      "\n",
      "Test set: Average LogProb: -10.127775\n",
      "\n",
      "Train Epoch: 57 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.464112\tX1: 10.390285 \tY: 244.787231 \tTotal: -268.641602\n",
      "\n",
      "Test set: Average LogProb: -10.127830\n",
      "\n",
      "Train Epoch: 58 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.333858\tX1: 12.442332 \tY: 335.245667 \tTotal: -363.021851\n",
      "\n",
      "Test set: Average LogProb: -10.127893\n",
      "\n",
      "Train Epoch: 59 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.039947\tX1: 11.989658 \tY: 234.915726 \tTotal: -261.945312\n",
      "\n",
      "Test set: Average LogProb: -10.126853\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 60 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.243242\tX1: 12.357672 \tY: 274.813293 \tTotal: -303.414215\n",
      "\n",
      "Test set: Average LogProb: -10.128203\n",
      "\n",
      "Train Epoch: 61 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.519671\tX1: 13.163614 \tY: 369.364716 \tTotal: -396.048035\n",
      "\n",
      "Test set: Average LogProb: -10.127359\n",
      "\n",
      "Train Epoch: 62 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 17.187490\tX1: 13.142471 \tY: 264.145294 \tTotal: -294.475250\n",
      "\n",
      "Test set: Average LogProb: -10.128044\n",
      "\n",
      "Train Epoch: 63 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.634048\tX1: 12.089023 \tY: 276.650055 \tTotal: -302.373138\n",
      "\n",
      "Test set: Average LogProb: -10.127520\n",
      "\n",
      "Train Epoch: 64 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.695910\tX1: 13.719831 \tY: 274.863159 \tTotal: -303.278900\n",
      "\n",
      "Test set: Average LogProb: -10.127322\n",
      "\n",
      "Train Epoch: 65 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.502550\tX1: 11.932933 \tY: 238.948669 \tTotal: -265.384186\n",
      "\n",
      "Test set: Average LogProb: -10.128650\n",
      "\n",
      "Train Epoch: 66 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.089672\tX1: 14.253258 \tY: 314.771179 \tTotal: -344.114136\n",
      "\n",
      "Test set: Average LogProb: -10.127597\n",
      "\n",
      "Train Epoch: 67 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.183830\tX1: 13.437281 \tY: 342.996216 \tTotal: -369.617310\n",
      "\n",
      "Test set: Average LogProb: -10.127741\n",
      "\n",
      "Train Epoch: 68 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.966433\tX1: 14.339436 \tY: 323.202606 \tTotal: -351.508484\n",
      "\n",
      "Test set: Average LogProb: -10.128361\n",
      "\n",
      "Train Epoch: 69 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.906793\tX1: 10.019979 \tY: 395.186829 \tTotal: -420.113617\n",
      "\n",
      "Test set: Average LogProb: -10.128273\n",
      "\n",
      "Train Epoch: 70 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.499391\tX1: 10.836735 \tY: 330.492920 \tTotal: -356.829041\n",
      "\n",
      "Test set: Average LogProb: -10.128015\n",
      "\n",
      "Train Epoch: 71 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.331690\tX1: 11.072188 \tY: 263.190125 \tTotal: -288.594025\n",
      "\n",
      "Test set: Average LogProb: -10.127666\n",
      "\n",
      "Train Epoch: 72 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.933699\tX1: 12.153563 \tY: 282.866455 \tTotal: -308.953705\n",
      "\n",
      "Test set: Average LogProb: -10.127192\n",
      "\n",
      "Train Epoch: 73 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.658298\tX1: 11.466564 \tY: 272.302338 \tTotal: -298.427185\n",
      "\n",
      "Test set: Average LogProb: -10.127909\n",
      "\n",
      "Train Epoch: 74 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.612084\tX1: 8.080481 \tY: 358.673340 \tTotal: -379.365906\n",
      "\n",
      "Test set: Average LogProb: -10.127964\n",
      "\n",
      "Train Epoch: 75 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.495270\tX1: 11.101418 \tY: 334.044861 \tTotal: -358.641571\n",
      "\n",
      "Test set: Average LogProb: -10.127169\n",
      "\n",
      "Train Epoch: 76 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 10.727715\tX1: 13.548038 \tY: 266.829163 \tTotal: -291.104950\n",
      "\n",
      "Test set: Average LogProb: -10.127069\n",
      "\n",
      "Train Epoch: 77 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 11.347328\tX1: 10.144997 \tY: 310.624756 \tTotal: -332.117065\n",
      "\n",
      "Test set: Average LogProb: -10.128020\n",
      "\n",
      "Train Epoch: 78 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.577332\tX1: 11.899036 \tY: 323.967072 \tTotal: -350.443420\n",
      "\n",
      "Test set: Average LogProb: -10.126798\n",
      "\n",
      "best is saved\n",
      "Train Epoch: 79 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.468381\tX1: 11.371130 \tY: 360.552551 \tTotal: -385.392029\n",
      "\n",
      "Test set: Average LogProb: -10.128100\n",
      "\n",
      "Train Epoch: 80 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.571617\tX1: 10.659158 \tY: 282.343414 \tTotal: -307.574188\n",
      "\n",
      "Test set: Average LogProb: -10.127232\n",
      "\n",
      "Train Epoch: 81 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.875275\tX1: 11.759270 \tY: 327.865692 \tTotal: -356.500214\n",
      "\n",
      "Test set: Average LogProb: -10.128140\n",
      "\n",
      "Train Epoch: 82 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.122685\tX1: 14.995782 \tY: 292.504761 \tTotal: -320.623230\n",
      "\n",
      "Test set: Average LogProb: -10.127364\n",
      "\n",
      "Train Epoch: 83 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.133055\tX1: 13.648590 \tY: 293.331604 \tTotal: -323.113220\n",
      "\n",
      "Test set: Average LogProb: -10.127146\n",
      "\n",
      "Train Epoch: 84 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.296608\tX1: 12.356740 \tY: 204.088135 \tTotal: -230.741470\n",
      "\n",
      "Test set: Average LogProb: -10.127564\n",
      "\n",
      "Train Epoch: 85 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 14.552739\tX1: 11.865149 \tY: 244.115250 \tTotal: -270.533142\n",
      "\n",
      "Test set: Average LogProb: -10.127199\n",
      "\n",
      "Train Epoch: 86 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.135035\tX1: 13.107453 \tY: 341.962128 \tTotal: -370.204620\n",
      "\n",
      "Test set: Average LogProb: -10.127177\n",
      "\n",
      "Train Epoch: 87 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 16.034382\tX1: 13.946695 \tY: 341.199432 \tTotal: -371.180511\n",
      "\n",
      "Test set: Average LogProb: -10.127004\n",
      "\n",
      "Train Epoch: 88 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.235680\tX1: 11.645424 \tY: 382.669403 \tTotal: -406.550537\n",
      "\n",
      "Test set: Average LogProb: -10.127407\n",
      "\n",
      "Train Epoch: 89 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 11.911497\tX1: 10.879381 \tY: 292.241394 \tTotal: -315.032257\n",
      "\n",
      "Test set: Average LogProb: -10.127820\n",
      "\n",
      "Train Epoch: 90 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 10.770885\tX1: 12.389452 \tY: 262.622620 \tTotal: -285.782959\n",
      "\n",
      "Test set: Average LogProb: -10.127636\n",
      "\n",
      "Train Epoch: 91 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 10.128995\tX1: 10.068768 \tY: 244.404953 \tTotal: -264.602722\n",
      "\n",
      "Test set: Average LogProb: -10.127974\n",
      "\n",
      "Train Epoch: 92 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 12.469424\tX1: 11.761746 \tY: 288.783234 \tTotal: -313.014404\n",
      "\n",
      "Test set: Average LogProb: -10.128151\n",
      "\n",
      "Train Epoch: 93 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.456558\tX1: 13.062866 \tY: 246.296127 \tTotal: -274.815552\n",
      "\n",
      "Test set: Average LogProb: -10.127607\n",
      "\n",
      "Train Epoch: 94 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.874230\tX1: 11.031487 \tY: 264.691315 \tTotal: -289.597076\n",
      "\n",
      "Test set: Average LogProb: -10.128292\n",
      "\n",
      "Train Epoch: 95 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.249031\tX1: 11.287361 \tY: 259.137634 \tTotal: -283.674011\n",
      "\n",
      "Test set: Average LogProb: -10.127674\n",
      "\n",
      "Train Epoch: 96 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 11.996584\tX1: 13.647806 \tY: 349.627319 \tTotal: -375.271729\n",
      "\n",
      "Test set: Average LogProb: -10.127535\n",
      "\n",
      "Train Epoch: 97 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 15.088558\tX1: 14.419721 \tY: 266.101837 \tTotal: -295.610107\n",
      "\n",
      "Test set: Average LogProb: -10.128485\n",
      "\n",
      "Train Epoch: 98 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 13.790195\tX1: 14.000095 \tY: 260.674286 \tTotal: -288.464600\n",
      "\n",
      "Test set: Average LogProb: -10.127445\n",
      "\n",
      "Train Epoch: 99 [0/800 (0.0%)]\t| -LogProb X6: -0.000000\tX5: -0.000000\tX4: -0.000000\tX3: -0.000000 \tX2: 17.969139\tX1: 15.451050 \tY: 274.835602 \tTotal: -308.255798\n",
      "\n",
      "Test set: Average LogProb: -10.128230\n",
      "\n",
      "==> Best LogProb: -10.126798, Time: 0.46 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(scm_partial,save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5426d69-16ea-479d-8492-070bdb6fb285",
   "metadata": {},
   "source": [
    "### **Inference**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9274aa80-18c6-46cc-8eb0-3bbb25733f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "\n",
    "from utils import mkdir\n",
    "\n",
    "\n",
    "def do_inference(model_name,intervention):\n",
    "    kwargs = {}\n",
    "    dataset = get_features_dataset(inference=True, dim=dim) #random_seed=9)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    model = model_name\n",
    "    saved_in_path= os.path.join(save,str(model_name.__class__.__name__)) \n",
    "    model_path = os.path.join(saved_in_path, 'flow_best.pth.tar')\n",
    "    checkpoint = torch.load(model_path)\n",
    "    print(model_path)\n",
    "    print('==> Best LogProb: {:.6f}\\n'.format(checkpoint['best_loss']))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print('intervention------',intervention)\n",
    "    #model.clear()\n",
    "    #feature = FeaturesInference(args, model, loader, dataset.data, dataset.covariates_dict)\n",
    "    #feature.inference()\n",
    "    Counterfactuals={'X_6':[],'X_5':[],'X_4':[],'X_3':[],'X_2':[],'X_1':[],'Y':[]}\n",
    "    #print('before starting---',Counterfactuals)\n",
    "    start_time=time.time()\n",
    "    for batch_idx, (X6, X5, X4, X3, X2, X1,Y,id) in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "                data = {'X6': X6,'X5': X5,'X4': X4, 'X3': X3, 'X2': X2, 'X1': X1,'Y':Y}\n",
    "                #print('intervention',intervention)\n",
    " \n",
    "                samples = model.counterfactual(obs=data, intervention=intervention, num_particles=particles)\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print('Inference: [{}/{} ({:.1f}%)]\\tIntervention: {}'.format(\n",
    "                        batch_idx * len(X4), len(loader.dataset), 100. * batch_idx / len(loader), intervention))\n",
    "                    \n",
    "                Counterfactuals['X_6'].append(np.array(samples['X_6']))\n",
    "                Counterfactuals['X_5'].append(np.array(samples['X_5']))\n",
    "                Counterfactuals['X_4'].append(np.array(samples['X_4']))\n",
    "                Counterfactuals['X_3'].append(np.array(samples['X_3']))\n",
    "                Counterfactuals['X_2'].append(np.array(samples['X_2'])) \n",
    "                Counterfactuals['X_1'].append(np.array(samples['X_1']))\n",
    "                Counterfactuals['Y'].append(np.array(samples['Y']))\n",
    "    print(' Time taken: {:.2f} secs'.format( (time.time()-start_time)))\n",
    "     \n",
    "    Counterfactuals['X_6']=np.vstack(Counterfactuals['X_6']).reshape(sample_size)\n",
    "    Counterfactuals['X_5']=np.vstack(Counterfactuals['X_5']).reshape(sample_size)            \n",
    "    Counterfactuals['X_4']=np.vstack(Counterfactuals['X_4']).reshape(sample_size)\n",
    "    Counterfactuals['X_3']=np.vstack(Counterfactuals['X_3']).reshape(sample_size)\n",
    "    Counterfactuals['X_2']=np.vstack(Counterfactuals['X_2']).reshape(sample_size)\n",
    "    Counterfactuals['X_1']=np.vstack(Counterfactuals['X_1']).reshape(sample_size)\n",
    "    Counterfactuals['Y']=np.vstack(Counterfactuals['Y']).reshape(sample_size)  \n",
    "    \n",
    "    return Counterfactuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578b765c-15ba-438a-966b-e6a2520c8f76",
   "metadata": {},
   "source": [
    "### *generate counterfactuals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8237095-7b46-4af9-be76-eb366f9af200",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "dim=1\n",
    "particles=32\n",
    "log_interval=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d537363-fd09-4006-82b0-42c71cdc7cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43bae61e-7634-4e57-b2c7-49cb98b1b264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.505145994173287"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j= 10\n",
    "intervention='do(X5='+str(j)+')'\n",
    "X_6,X_5, X_4, X_3, X_2, X_1, Y= SCM(True,j)      #True Counterfactuals\n",
    "np.mean(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dc7f088-a21e-432f-b8f6-4a2eac0f0b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "logs/ConditionalSCM/flow_best.pth.tar\n",
      "==> Best LogProb: -12.209026\n",
      "\n",
      "intervention------ do(X5=10)\n",
      "Inference: [0/1000 (0.0%)]\tIntervention: do(X5=10)\n",
      " Time taken: 6.75 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27.505146"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCM_counterfactuals=do_inference(scm,intervention)\n",
    "np.mean(SCM_counterfactuals['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44402f82-daa1-4468-a380-6c34a19733af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> started preprocessing data ...\n",
      "==> finished preprocessing data ...\n",
      "logs/ConditionalSCM_partial/flow_best.pth.tar\n",
      "==> Best LogProb: -10.126798\n",
      "\n",
      "intervention------ do(X5=10)\n",
      "Inference: [0/1000 (0.0%)]\tIntervention: do(X5=10)\n"
     ]
    }
   ],
   "source": [
    "SCM_partial_counterfactuals=do_inference(scm_partial,intervention)\n",
    "np.mean(SCM_partial_counterfactuals['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85245f31-7dca-4b64-8eef-a39526f3166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "data={'Y':Y,'partial':SCM_partial_counterfactuals['Y'],'full':SCM_counterfactuals['Y']}\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.kdeplot(data['Y'], ax=ax, color='red', fill=True,bw_adjust=2,legend=True,label='True',alpha=0.1)\n",
    "sns.kdeplot(data['full'], ax=ax, color='yellow',linestyle='-' ,fill=False,bw_adjust=2,legend=True,label='full model')\n",
    "sns.kdeplot(data['partial'], ax=ax,color='black',linestyle='--' ,fill=False,bw_adjust=2,legend=True,label='partial model')\n",
    "plt.legend()\n",
    "plt.ylabel('Density',fontweight='bold')\n",
    "plt.title(r'$P(Y_{X_{5}\\leftarrow %.2f}|\\mathbf{X}=\\mathbf{x}^{obs},Y=y^{obs})$'%j,fontsize=15)\n",
    "fig.savefig('toy_model.pdf',format='pdf',dpi=600)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5354f68c-0595-44fa-b56d-baa59359f225",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba069cf-595c-4025-b7f1-57c4d27ceca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "intervention_set=np.random.uniform(-10,10,100)\n",
    "error_full=[]\n",
    "error_partial=[]\n",
    "\n",
    "for j in intervention_set:\n",
    "    intervention='do(X5='+str(j)+')'\n",
    "    SCM_counterfactuals=do_inference(scm,intervention)\n",
    "    SCM_partial_counterfactuals=do_inference(scm_partial,intervention)\n",
    "    \n",
    "    X_6, X_5, X_4, X_3, X_2, X_1, Y= SCM(True,j)\n",
    "    \n",
    "    error_full.append( np.linalg.norm(Y-SCM_counterfactuals['Y'])/sample_size)\n",
    "    error_partial.append(np.linalg.norm( Y-SCM_partial_counterfactuals['Y'])/sample_size)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c94a68d-e73d-4240-b821-fcf76bf73852",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame({'intervention':intervention_set,'error_partial':error_partial,'error_full': error_full})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08999493-472b-450e-8506-8d683fe458d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "#plt.scatter(intervention_set,np.exp(np.array(error_partial)),label='partial model',color='black',ls='-',marker='o')\n",
    "#plt.scatter(intervention_set,np.exp(np.array(error_full)),label='full model',color='yellow',ls='--',marker='*')\n",
    "sns.scatterplot(data=data,x='intervention',y='error_partial',label='partial model',color='black',marker='o',alpha=0.5)\n",
    "sns.scatterplot(data=data,x='intervention',y='error_full',label='full model',color='red',marker='*',alpha=0.5)\n",
    "\n",
    "plt.xlabel(r'Intervention value on $X_{5}$ ',fontweight='bold')\n",
    "plt.ylabel('mean squared error',fontweight='bold')\n",
    "plt.legend()\n",
    "fig.savefig('error_synthetic_data.pdf',format='pdf',dpi=1200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
