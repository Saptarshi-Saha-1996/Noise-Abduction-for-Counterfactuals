{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea3c23b-6890-43c5-b573-a57722438de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17b803c9-d824-4424-a702-b34fdadd83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(97)\n",
    "\n",
    "sample_size=1000\n",
    "epsilon_1=np.random.uniform(0,1,(sample_size,3))\n",
    "epsilon_2=np.random.uniform(0,1,(sample_size,2))\n",
    "epsilon_3=np.random.uniform(0,1,(sample_size,2))\n",
    "epsilon=np.concatenate((epsilon_1,epsilon_2,epsilon_3),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc148fb-3364-4802-bdf1-d1307d361ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SCM(intervention=False,*args):\n",
    "    X_4=epsilon[:,4]\n",
    "    X_6=epsilon[:,6]-1\n",
    "    if not intervention:\n",
    "        X_5=3*X_6+epsilon[:,5]-1\n",
    "    else:\n",
    "        X_5=X_5+3                    #np.full(sample_size,args)\n",
    "    X_2=X_5-epsilon[:,2]\n",
    "    X_3= (50+epsilon[:,3])*(X_4>0.5)+ (-50-epsilon[:,3])*(X_4<=0.5)\n",
    "    \n",
    "    X_1=X_6-X_5+3*epsilon[:,1]\n",
    "    Y=X_1+2*X_2-3*X_3+epsilon[:,0]\n",
    "    return X_6,X_5,X_4,X_3,X_2,X_1,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3becaa3d-11c3-494c-8eb1-84da2e4980b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_6,X_5,X_4,X_3,X_2,X_1,Y= SCM(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06f8716d-dad4-4f03-b33e-3f4d8dc6c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.misc as m\n",
    "from torch.utils import data\n",
    "from math import isnan, isinf\n",
    "\n",
    "\n",
    "\n",
    "class featuresDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,dim = 1,\n",
    "        inference = False, train = True,\n",
    "        train_ratio = 0.8,\n",
    "        #random_seed = 67,\n",
    "    ):\n",
    "        super(featuresDataset, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.random_seed =   69 #random_seed\n",
    "        self.inference = inference\n",
    "\n",
    "        self.data, self.y = [], []\n",
    "        print('==> started preprocessing data ...')\n",
    "        for index in range(sample_size):\n",
    "            self.data.append(dict(\n",
    "                X6 =  np.asarray(X_6[index]).astype(np.float32),\n",
    "                X5 =  np.asarray(X_5[index]).astype(np.float32),\n",
    "                X4 =  np.asarray(X_4[index]).astype(np.float32),\n",
    "                X3 =  np.asarray(X_3[index]).astype(np.float32),\n",
    "                X2 =  np.asarray(X_2[index]).astype(np.float32),\n",
    "                X1 =  np.asarray(X_1[index]).astype(np.float32),\n",
    "                Y  =  np.asarray(Y[index]).astype(np.float32), \n",
    "                sample_id= index\n",
    "            ))\n",
    "        print('==> finished preprocessing data ...')\n",
    "\n",
    "        if not self.inference:                                                      #when inference is flase \n",
    "            self.ids = np.arange(0, len(self.data))           #img_idxes --> ids\n",
    "            np.random.seed(self.random_seed)\n",
    "            np.random.shuffle(self.ids)\n",
    "            last_train_sample = int(len(self.ids) * train_ratio)\n",
    "            if train:                                                               #when training is true\n",
    "                self.ids = self.ids[:last_train_sample]\n",
    "            else:\n",
    "                self.ids = self.ids[last_train_sample:]\n",
    "        else:                                                                       #when inference is true\n",
    "            self.ids = np.arange(0, len(self.data))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id_ = self.ids[index]\n",
    "        info = self.data[id_]\n",
    "        \n",
    "        id= info['sample_id']\n",
    "        X6 = torch.from_numpy(info['X6'])\n",
    "        X5 = torch.from_numpy(info['X5'])\n",
    "        X4 = torch.from_numpy(info['X4'])\n",
    "        X3 = torch.from_numpy(info['X3'])\n",
    "        X2 = torch.from_numpy(info['X2'])\n",
    "        X1 = torch.from_numpy(info['X1'])\n",
    "        Y = torch.from_numpy(info['Y'])\n",
    "        #X4 = torch.tensor().float()\n",
    "        #age = torch.tensor(info['age']).unsqueeze(-1).float()\n",
    "        \n",
    "\n",
    "        if self.inference:\n",
    "            return X6, X5, X4,  X3, X2,  X1, Y, id\n",
    "        else:\n",
    "            return X6, X5, X4,  X3, X2,  X1, Y\n",
    "\n",
    "\n",
    "def get_features_dataset(\n",
    "    dim=1,\n",
    "    inference=False,\n",
    "    train_ratio=0.8,\n",
    "    #random_seed=67,\n",
    "    ):\n",
    "\n",
    "    if not inference:\n",
    "        dataset_train = featuresDataset(train=True, dim=dim,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "        dataset_test = featuresDataset(train=False,  dim=dim,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "\n",
    "        return dataset_train, dataset_test\n",
    "    else:\n",
    "        dataset = featuresDataset(inference=True,  dim=dim,\n",
    "            train_ratio=train_ratio) #random_seed=random_seed)\n",
    "\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "371dfb75-d173-42a9-bbe7-d1112901592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Independent\n",
    "from pyro.infer.reparam.transform import TransformReparam\n",
    "from pyro.distributions.torch_transform import ComposeTransformModule\n",
    "from pyro.nn import PyroModule, pyro_method\n",
    "from pyro.distributions import Normal,TransformedDistribution\n",
    "from pyro.distributions.transforms import Spline, AffineTransform, ExpTransform, ComposeTransform, spline_coupling \n",
    "from pyro.distributions.transforms import conditional_spline\n",
    "from pyro.distributions.conditional import ConditionalTransformedDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4ffb71a-73f0-4fcf-b92d-6c9fdc068ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSCM(PyroModule):\n",
    "    def __init__(self,context_dim=2,normalize=True,spline_bins=8,spline_order='linear'):\n",
    "        super(ConditionalSCM, self).__init__()\n",
    "        self.context_dim = context_dim\n",
    "        self.spline_bins = spline_bins\n",
    "        self.spline_order = spline_order\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "        \n",
    "        #Flows ---------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # X_4 flows\n",
    "        self.X4_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        self.X4_flow_transforms = ComposeTransform([self.X4_flow_component])\n",
    "        \n",
    "        # X_6 flows\n",
    "        self.X6_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        self.X6_flow_transforms = ComposeTransform([self.X6_flow_component])\n",
    "        \n",
    "        #X_5 flows\n",
    "        self.X5_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X5_flow_transforms = ComposeTransformModule([self.X5_flow_component])\n",
    "        \n",
    "        #X_2 flows \n",
    "        self.X2_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X2_flow_transforms = ComposeTransformModule([self.X2_flow_component])\n",
    "        \n",
    "        #X_3 flows \n",
    "        self.X3_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X3_flow_transforms = ComposeTransformModule([self.X3_flow_component])\n",
    "        \n",
    "        \n",
    "        #X_1 flows \n",
    "        self.X1_flow_component = conditional_spline(1, context_dim=2, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X1_flow_transforms = ComposeTransformModule([self.X1_flow_component])\n",
    "        \n",
    "        #Y flows\n",
    "        self.Y_flow_component = conditional_spline(1, context_dim=3, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.Y_flow_transforms = ComposeTransformModule([self.Y_flow_component])\n",
    "        \n",
    "       \n",
    "\n",
    "            \n",
    "    def pgm_model(self):\n",
    "        loc=torch.tensor([0.0])\n",
    "        scale=torch.tensor([1.0])\n",
    "        # X4\n",
    "        self.X4_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X4_dist=TransformedDistribution(self.X4_base_dist, self.X4_flow_transforms)\n",
    "        self.X4 = pyro.sample('X4', self.X4_dist,)\n",
    "        # X6\n",
    "        self.X6_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X6_dist=TransformedDistribution(self.X6_base_dist, self.X6_flow_transforms)\n",
    "        self.X6 = pyro.sample('X6', self.X6_dist,)\n",
    "        #X5\n",
    "        context_X5=torch.cat([self.X6],-1)\n",
    "        self.X5_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X5_dist = ConditionalTransformedDistribution(self.X5_base_dist, self.X5_flow_transforms).condition(context_X5)\n",
    "        self.X5 = pyro.sample('X5', self.X5_dist)\n",
    "        #X3\n",
    "        context_X3=torch.cat([self.X4],-1)\n",
    "        self.X3_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X3_dist = ConditionalTransformedDistribution(self.X3_base_dist, self.X3_flow_transforms).condition(context_X3)\n",
    "        self.X3 = pyro.sample('X3', self.X3_dist)\n",
    "        #X2\n",
    "        context_X2=torch.cat([self.X5],-1)\n",
    "        self.X2_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X2_dist = ConditionalTransformedDistribution(self.X2_base_dist, self.X2_flow_transforms).condition(context_X2)\n",
    "        self.X2 = pyro.sample('X2', self.X2_dist)\n",
    "        #X1\n",
    "        context_X1=torch.cat([self.X5,self.X6],-1)\n",
    "        self.X1_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X1_dist = ConditionalTransformedDistribution(self.X1_base_dist, self.X1_flow_transforms).condition(context_X1)\n",
    "        self.X1 = pyro.sample('X1', self.X1_dist)\n",
    "        # Y\n",
    "        context_Y=torch.cat([self.X3,self.X2,self.X1],-1)\n",
    "        self.Y_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.Y_dist = ConditionalTransformedDistribution(self.Y_base_dist, self.Y_flow_transforms).condition(context_Y)\n",
    "        self.Y = pyro.sample('Y', self.Y_dist)\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X6,X5, X4,X3,X2,X1,Y ):\n",
    "        self.pgm_model()\n",
    "        X6_logp = self.X6_dist.log_prob(X6)\n",
    "        X5_logp = self.X5_dist.log_prob(X5)\n",
    "        X4_logp = self.X4_dist.log_prob(X4)\n",
    "        X3_logp = self.X3_dist.log_prob(X3)\n",
    "        X2_logp = self.X2_dist.log_prob(X2)\n",
    "        X1_logp = self.X1_dist.log_prob(X1)\n",
    "        Y_logp = self.Y_dist.log_prob(Y)\n",
    "        return {'X_6':X6_logp,'X_5':X5_logp,'X_4': X4_logp, 'X_3': X3_logp, 'X_2': X2_logp, 'X_1': X1_logp,'Y': Y_logp}\n",
    "\n",
    "    def clear(self):\n",
    "        self.X5_dist.clear_cache()\n",
    "        self.X3_dist.clear_cache()\n",
    "        self.X2_dist.clear_cache()\n",
    "        self.X1_dist.clear_cache()\n",
    "        self.Y_dist.clear_cache()\n",
    "        \n",
    "    def model(self):\n",
    "        self.pgm_model()\n",
    "        return self.X6, self.X5, self.X4, self.X3, self.X2, self.X1, self.Y\n",
    "\n",
    "    def scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam()\n",
    "            else:\n",
    "                return None\n",
    "        return pyro.poutine.reparam(self.model, config=config)(*args, **kwargs)\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.model()\n",
    "        return (*samples,)\n",
    "\n",
    "    def sample_scm(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.scm()\n",
    "        return (*samples,)\n",
    "    \n",
    "    def infer_exogeneous(self, **obs):\n",
    "        cond_sample = pyro.condition(self.sample, data=obs)\n",
    "        cond_trace = pyro.poutine.trace(cond_sample).get_trace(obs['X1'].shape[0])\n",
    "        output = {}\n",
    "        for name, node in cond_trace.nodes.items():\n",
    "            if 'fn' not in node.keys():\n",
    "                continue\n",
    "            fn = node['fn']\n",
    "            if isinstance(fn, Independent):\n",
    "                fn = fn.base_dist\n",
    "\n",
    "            if isinstance(fn, TransformedDistribution):\n",
    "                output[name + '_base'] = ComposeTransform(fn.transforms).inv(node['value'])\n",
    "        return output\n",
    "\n",
    "    def counterfactual(self, obs,intervention, num_particles=1):\n",
    "        counterfactuals = []\n",
    "\n",
    "        for _ in range(num_particles):\n",
    "            exogeneous = self.infer_exogeneous(**obs)\n",
    "            #condition= {intervention.split('=')[0].split('(')[-1] :torch.full_like( obs['X4'],float(intervention.split('=')[-1][:-1]))}\n",
    "            print('intervention',intervention)\n",
    "            if intervention != None:\n",
    "                X5=obs['X5']\n",
    "                print('X5_before',X5)\n",
    "                print(intervention.split('(')[-1][:-1])\n",
    "                print('X5_after',X5)\n",
    "                condition={intervention.split('=')[0].split('(')[-1] :X5*torch.ones_like(obs['X4'])}\n",
    "            print('inside class: condition',condition)\n",
    "            \n",
    "            \n",
    "            \n",
    "            counter = pyro.poutine.do(pyro.poutine.condition(self.sample_scm, data=exogeneous), data=condition)(obs['X1'].shape[0])\n",
    "            counterfactuals += [counter]\n",
    "            #print('helper',helper)\n",
    "            #print('helper',helper)\n",
    "\n",
    "        return {k: v for k, v in zip(('X_6','X_5','X_4', 'X_3', 'X_2', 'X_1','Y'), (torch.stack(c).mean(0) for c in zip(*counterfactuals)))}\n",
    "\n",
    "\n",
    "def conditionalscm():\n",
    "    model = ConditionalSCM(context_dim=2,normalize=True,spline_bins=8,spline_order='linear')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea93c39e-95d9-41b0-b1b7-a5790c0e63cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scm=conditionalscm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5518a3c-3297-4177-8dd7-e4bb5b8ceb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSCM_partial(PyroModule):\n",
    "    def __init__(self,context_dim=2,normalize=True,spline_bins=8,spline_order='linear'):\n",
    "        super(ConditionalSCM_partial, self).__init__()\n",
    "        self.context_dim = context_dim\n",
    "        self.spline_bins = spline_bins\n",
    "        self.spline_order = spline_order\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "        \n",
    "        #Flows ---------------------------------------------------------------------------------------------\n",
    "        \n",
    "        # X_4 flows\n",
    "        #self.X4_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        #self.X4_flow_transforms = ComposeTransform([self.X4_flow_component])\n",
    "        \n",
    "        # X_3 flows\n",
    "        #self.X3_flow_component = Spline(1, count_bins=self.spline_bins)\n",
    "        #self.X3_flow_transforms = ComposeTransform([self.X3_flow_component])\n",
    "        \n",
    "        #X_2 flows \n",
    "        self.X2_flow_component = conditional_spline(1, context_dim=1, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X2_flow_transforms = ComposeTransformModule([self.X2_flow_component])\n",
    "        \n",
    "        #X_1 flows \n",
    "        self.X1_flow_component = conditional_spline(1, context_dim=2, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.X1_flow_transforms = ComposeTransformModule([self.X1_flow_component])\n",
    "        \n",
    "        #Y flows\n",
    "        self.Y_flow_component = conditional_spline(1, context_dim=3, count_bins=self.spline_bins, order=self.spline_order)\n",
    "        self.Y_flow_transforms = ComposeTransformModule([self.Y_flow_component])\n",
    "        \n",
    "       \n",
    "\n",
    "            \n",
    "    def pgm_model(self):\n",
    "        loc=torch.tensor([0.0])\n",
    "        scale=torch.tensor([1.0])\n",
    "         # X4\n",
    "        self.X4_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X4_dist=TransformedDistribution(self.X4_base_dist, self.X4_flow_transforms)\n",
    "        self.X4 = pyro.sample('X4', self.X4_dist,)\n",
    "        # X6\n",
    "        self.X6_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X6_dist=TransformedDistribution(self.X6_base_dist, self.X6_flow_transforms)\n",
    "        self.X6 = pyro.sample('X6', self.X6_dist,)\n",
    "        #X5\n",
    "        #context_X5=torch.cat([self.X6],-1)\n",
    "        self.X5_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X5_dist = ConditionalTransformedDistribution(self.X5_base_dist, self.X5_flow_transforms).condition(context_X5)\n",
    "        self.X5 = pyro.sample('X5', self.X5_dist)\n",
    "        #X3\n",
    "        #context_X3=torch.cat([self.X4],-1)\n",
    "        self.X3_dist = Normal(loc,scale).to_event(1)\n",
    "        #self.X3_dist = ConditionalTransformedDistribution(self.X3_base_dist, self.X3_flow_transforms).condition(context_X3)\n",
    "        self.X3 = pyro.sample('X3', self.X3_dist)\n",
    "        \n",
    "        #X2\n",
    "        context_X2=torch.cat([self.X5],-1)\n",
    "        self.X2_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X2_dist = ConditionalTransformedDistribution(self.X2_base_dist, self.X2_flow_transforms).condition(context_X2)\n",
    "        self.X2 = pyro.sample('X2', self.X2_dist)\n",
    "        #X1\n",
    "        context_X1=torch.cat([self.X5,self.X6],-1)\n",
    "        self.X1_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.X1_dist = ConditionalTransformedDistribution(self.X1_base_dist, self.X1_flow_transforms).condition(context_X1)\n",
    "        self.X1 = pyro.sample('X1', self.X1_dist)\n",
    "        # Y\n",
    "        context_Y=torch.cat([self.X3,self.X2,self.X1],-1)\n",
    "        self.Y_base_dist = Normal(loc,scale).to_event(1)\n",
    "        self.Y_dist = ConditionalTransformedDistribution(self.Y_base_dist, self.Y_flow_transforms).condition(context_Y)\n",
    "        self.Y = pyro.sample('Y', self.Y_dist)\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, X6, X5, X4,X3,X2,X1,Y ):\n",
    "        self.pgm_model()\n",
    "        Y_logp = self.Y_dist.log_prob(Y)\n",
    "        X1_logp = self.X1_dist.log_prob(X1)\n",
    "        X2_logp = self.X2_dist.log_prob(X2)\n",
    "        \n",
    "        X3_logp = torch.zeros(Y_logp.shape)                                  #self.X3_dist.log_prob(X3)                  \n",
    "        X4_logp = torch.zeros(Y_logp.shape)                                  #self.X4_dist.log_prob(X4)                                \n",
    "        X5_logp = torch.zeros(Y_logp.shape)                                  #self.X2_dist.log_prob(X2)\n",
    "        X6_logp = torch.zeros(Y_logp.shape)                                  #self.X1_dist.log_prob(X1)\n",
    "        \n",
    "        return {'X_6': X6_logp,'X_5': X5_logp,'X_4': X4_logp, 'X_3': X3_logp, 'X_2': X2_logp, 'X_1': X1_logp,'Y': Y_logp}\n",
    "\n",
    "    def clear(self):\n",
    "        self.X2_dist.clear_cache()\n",
    "        self.X1_dist.clear_cache()\n",
    "        self.Y_dist.clear_cache()\n",
    "        \n",
    "    def model(self):\n",
    "        self.pgm_model()\n",
    "        return self.X6, self.X5, self.X4, self.X3, self.X2, self.X1, self.Y\n",
    "\n",
    "    def scm(self, *args, **kwargs):\n",
    "        def config(msg):\n",
    "            if isinstance(msg['fn'], TransformedDistribution):\n",
    "                return TransformReparam()\n",
    "            else:\n",
    "                return None\n",
    "        return pyro.poutine.reparam(self.model, config=config)(*args, **kwargs)\n",
    "\n",
    "    def sample(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.model()\n",
    "        return (*samples,)\n",
    "\n",
    "    def sample_scm(self, n_samples=1):\n",
    "        with pyro.plate('observations', n_samples):\n",
    "            samples = self.scm()\n",
    "        return (*samples,)\n",
    "    \n",
    "    def infer_exogeneous(self, **obs):\n",
    "        cond_sample = pyro.condition(self.sample, data=obs)\n",
    "        cond_trace = pyro.poutine.trace(cond_sample).get_trace(obs['X1'].shape[0])\n",
    "        output = {}\n",
    "        for name, node in cond_trace.nodes.items():\n",
    "            if 'fn' not in node.keys():\n",
    "                continue\n",
    "            fn = node['fn']\n",
    "            if isinstance(fn, Independent):\n",
    "                fn = fn.base_dist\n",
    "\n",
    "            if isinstance(fn, TransformedDistribution):\n",
    "                output[name + '_base'] = ComposeTransform(fn.transforms).inv(node['value'])\n",
    "        return output\n",
    "\n",
    "    def counterfactual(self, obs, intervention, num_particles=1):\n",
    "        counterfactuals = []\n",
    "        for _ in range(num_particles):\n",
    "            exogeneous = self.infer_exogeneous(**obs)\n",
    "            condition= {intervention.split('=')[0].split('(')[-1] :torch.full_like( obs['X4'],float(intervention.split('=')[-1][:-1]))}\n",
    "            condition['X3']=obs['X3']\n",
    "            condition['X4']=obs['X4']\n",
    "            condition['X6']=obs['X6']\n",
    "            #print('condition',condition)\n",
    "            counter = pyro.poutine.do(pyro.poutine.condition(self.sample_scm, data=exogeneous), data=condition)(obs['X1'].shape[0])\n",
    "            counterfactuals += [counter]\n",
    "        return {k: v for k, v in zip(('X_6','X_5','X_4', 'X_3', 'X_2', 'X_1','Y'), (torch.stack(c).mean(0) for c in zip(*counterfactuals)))}\n",
    "\n",
    "\n",
    "def conditionalscm_partial():\n",
    "    model = ConditionalSCM_partial(context_dim=2,normalize=True,spline_bins=8,spline_order='linear')\n",
    "    return model\n",
    "\n",
    "scm_partial=conditionalscm_partial()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fa56391-0bb5-4c5b-be0b-5c4070dc7fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X6': tensor([[0.2647],\n",
      "        [0.3470]]), 'X5': tensor([[0.1798],\n",
      "        [0.7652]]), 'X4': tensor([[0.0386],\n",
      "        [0.8339]]), 'X3': tensor([[0.1434],\n",
      "        [0.4066]]), 'X2': tensor([[0.3686],\n",
      "        [0.3074]]), 'X1': tensor([[0.4693],\n",
      "        [0.3676]]), 'Y': tensor([[0.3839],\n",
      "        [0.0733]])}\n"
     ]
    }
   ],
   "source": [
    "data = {'X6': torch.rand(2).reshape(-1,1),\n",
    "        'X5': torch.rand(2).reshape(-1,1),\n",
    "        'X4': torch.rand(2).reshape(-1,1), \n",
    "        'X3': torch.rand(2).reshape(-1,1), \n",
    "        'X2': torch.rand(2).reshape(-1,1),\n",
    "        'X1': torch.rand(2).reshape(-1,1),\n",
    "        'Y': torch.rand(2).reshape(-1,1)} \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc174329-fab6-463a-bb95-3642b4b509e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d67a15a-8ab5-4828-a2a6-0d7e956e76e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X4_base': tensor([[0.1730],\n",
      "        [0.3156]], grad_fn=<IndexPutBackward0>), 'X6_base': tensor([[1.1587],\n",
      "        [1.1678]], grad_fn=<IndexPutBackward0>), 'X5_base': tensor([[0.1414],\n",
      "        [0.7393]], grad_fn=<IndexPutBackward0>), 'X3_base': tensor([[0.5921],\n",
      "        [0.8854]], grad_fn=<IndexPutBackward0>), 'X2_base': tensor([[0.1971],\n",
      "        [0.3207]], grad_fn=<IndexPutBackward0>), 'X1_base': tensor([[0.8511],\n",
      "        [0.7581]], grad_fn=<IndexPutBackward0>), 'Y_base': tensor([[ 0.0982],\n",
      "        [-0.3546]], grad_fn=<IndexPutBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(scm.infer_exogeneous(**data))\n",
    "exogeneous=scm.infer_exogeneous(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b722fc0-bade-4066-b881-a02c02fdff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intervention do(X5=X5-5)\n",
      "inside class: condition {'X5': tensor([[-4.1694],\n",
      "        [-4.6086]])}\n"
     ]
    }
   ],
   "source": [
    "intervention='do(X5=X5-5)'\n",
    "if intervention !=None:\n",
    "    print('intervention',intervention)\n",
    "    X5=data['X5']\n",
    "    exec(intervention.split('(')[-1][:-1])\n",
    "    condition={intervention.split('=')[0].split('(')[-1] :X5*torch.ones_like(data['X4'])}\n",
    "print('inside class: condition',condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17e92659-c4de-425d-bcce-68191701e23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do(X5=3.4)\n",
      "intervention do(X5=3.4)\n",
      "X5_before tensor([[0.1798],\n",
      "        [0.7652]])\n",
      "X5_after tensor([[0.1798],\n",
      "        [0.7652]])\n",
      "inside class: condition {'X5': tensor([[0.1798],\n",
      "        [0.7652]])}\n",
      "{'X_6': tensor([[0.2647],\n",
      "        [0.3470]]), 'X_5': tensor([[0.1798],\n",
      "        [0.7652]]), 'X_4': tensor([[0.0386],\n",
      "        [0.8339]]), 'X_3': tensor([[0.1434],\n",
      "        [0.4066]], grad_fn=<MeanBackward1>), 'X_2': tensor([[0.3686],\n",
      "        [0.3074]], grad_fn=<MeanBackward1>), 'X_1': tensor([[0.4693],\n",
      "        [0.3676]], grad_fn=<MeanBackward1>), 'Y': tensor([[0.3839],\n",
      "        [0.0733]], grad_fn=<MeanBackward1>)}\n"
     ]
    }
   ],
   "source": [
    "intervention='do(X5=3.4)'\n",
    "#condition= {intervention.split('=')[0].split('(')[-1] :torch.full_like( data['X4'],float(intervention.split('=')[-1][:-1]))}\n",
    "#intervention='do(X5 =3)'\n",
    "\n",
    "\n",
    "print(intervention)\n",
    "print(scm.counterfactual(obs=data,intervention=intervention))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35736b5-b9ea-41dd-b6d9-987085e60328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_seed=69    #default 69\n",
    "batch_size=64\n",
    "test_batch_size=32\n",
    "lr=3e-4\n",
    "weight_decay=1e-4\n",
    "epochs=100\n",
    "lr_annealing=False\n",
    "log_interval=100\n",
    "#seed=17        #default 17\n",
    "save='./logs2'    #filepath to save training logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c21fc9-27ca-4d20-87a1-30b486552abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def milestone_step( optimizer, epoch):\n",
    "    if epoch in [epochs*0.5, epochs*0.75]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7193d39-3017-4a77-a1e0-04efe0c672d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "from utils import mkdir\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filepath):\n",
    "    mkdir(filepath)\n",
    "    torch.save(state, os.path.join(filepath, 'flow_ckpt.pth.tar'))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(filepath, 'flow_ckpt.pth.tar'), os.path.join(filepath, 'flow_best.pth.tar'))\n",
    "        print('best is saved')\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def train( model, optimizer, train_loader, epoch):\n",
    "    avg_loss = 0.\n",
    "    for batch_idx, (X6,X5,X4,X3, X2, X1,Y) in enumerate(train_loader):\n",
    "        X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "        optimizer.zero_grad()\n",
    "        log_p = model(X6, X5, X4, X3 ,X2, X1,Y)\n",
    "        loss = -torch.mean(log_p['X_6']+log_p['X_5']+log_p['X_4']+log_p['X_3']+log_p['X_2']+log_p['X_1'] +log_p['Y'])\n",
    "        avg_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.clear()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.1f}%)]\\t| -LogProb X6: {:.6f}\\tX5: {:.6f}\\tX4: {:.6f}\\tX3: {:.6f} \\tX2: {:.6f}\\tX1: {:.6f} \\tY: {:.6f} \\tTotal: {:.6f}'.format(epoch, batch_idx * len(X1),\n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader),\n",
    "                -torch.mean(log_p['X_6']).item(), -torch.mean(log_p['X_5']).item(),\n",
    "                -torch.mean(log_p['X_4']).item(), -torch.mean(log_p['X_3']).item(),\n",
    "                -torch.mean(log_p['X_2']).item(), -torch.mean(log_p['X_1']).item(),-torch.mean(log_p['Y']).item(), -loss.item()))\n",
    "            \n",
    "            \n",
    "\n",
    "def test(model, test_loader):\n",
    "    test_loss = 0.\n",
    "    for X6, X5, X4, X3, X2, X1,Y in test_loader:\n",
    "        with torch.no_grad():\n",
    "            X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "            log_p = model(X6, X5, X4, X3 ,X2, X1,Y)\n",
    "            test_loss += torch.mean(log_p['X_6']+log_p['X_5']+log_p['X_4']+log_p['X_3']+log_p['X_2']+log_p['X_1'] +log_p['Y'])\n",
    "                                    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average LogProb: {:.6f}\\n'.format(test_loss))\n",
    "    return test_loss\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "def main(model_name,filepath):\n",
    "    kwargs =  {}\n",
    "    dataset_train, dataset_test = get_features_dataset( dim=1) #random_seed=data_seed)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model = model_name\n",
    " \n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs, 0)\n",
    "\n",
    "    best_loss = -200.\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        if not lr_annealing:\n",
    "            milestone_step(optimizer, epoch)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        train( model, optimizer, train_loader, epoch)\n",
    "        loss = test(model, test_loader)\n",
    "        \n",
    "        is_best = loss > best_loss\n",
    "        best_loss = max(loss, best_loss)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_loss': best_loss,\n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }, is_best, filepath=filepath)\n",
    "\n",
    "    del model\n",
    "    print('==> Best LogProb: {:.6f}, Time: {:.2f} min\\n'.format(best_loss, (time.time()-start_time)/60.))\n",
    "    \n",
    "    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c497d77-6716-4b2f-acde-96870c4450f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,path):\n",
    "    mkdir(path)\n",
    "    path = os.path.join(path, str(model.__class__.__name__))\n",
    "    mkdir(path)\n",
    "    filepath=path\n",
    "    main(model,filepath) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2698ecd-f33a-4a89-81be-dee8cfaae0ff",
   "metadata": {},
   "source": [
    "## Trainingtrain_model(scm,save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77c81d-f97b-4c24-8eb9-b196259444fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(scm,save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3021b-97a4-4d31-8f48-1c8bafc8bb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(scm_partial,save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4df90-8800-44f8-8c75-a71d8c31d645",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8bc22-dcf2-4e92-a2db-6de2e58cc5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing as mp\n",
    "\n",
    "from utils import mkdir\n",
    "\n",
    "\n",
    "def do_inference(model_name,intervention):\n",
    "    kwargs = {}\n",
    "    dataset = get_features_dataset(inference=True, dim=dim) #random_seed=9)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    model = model_name\n",
    "    saved_in_path= os.path.join(save,str(model_name.__class__.__name__)) \n",
    "    model_path = os.path.join(saved_in_path, 'flow_best.pth.tar')\n",
    "    checkpoint = torch.load(model_path)\n",
    "    print(model_path)\n",
    "    print('==> Best LogProb: {:.6f}\\n'.format(checkpoint['best_loss']))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    print('intervention------',intervention)\n",
    "    #model.clear()\n",
    "    #feature = FeaturesInference(args, model, loader, dataset.data, dataset.covariates_dict)\n",
    "    #feature.inference()\n",
    "    Counterfactuals={'X_6':[],'X_5':[],'X_4':[],'X_3':[],'X_2':[],'X_1':[],'Y':[]}\n",
    "    #print('before starting---',Counterfactuals)\n",
    "    start_time=time.time()\n",
    "    for batch_idx, (X6, X5, X4, X3, X2, X1,Y,id) in enumerate(loader):\n",
    "            with torch.no_grad():\n",
    "                X6, X5, X4, X3, X2, X1,Y = X6.reshape([-1,1]), X5.reshape([-1,1]), X4.reshape([-1,1]), X3.reshape([-1,1]), X2.reshape([-1,1]), X1.reshape([-1,1]), Y.reshape([-1,1]) \n",
    "                data = {'X6': X6,'X5': X5,'X4': X4, 'X3': X3, 'X2': X2, 'X1': X1,'Y':Y}\n",
    "                #print('intervention',intervention)\n",
    " \n",
    "                samples = model.counterfactual(obs=data, intervention=intervention, num_particles=particles)\n",
    "                if batch_idx % log_interval == 0:\n",
    "                    print('Inference: [{}/{} ({:.1f}%)]\\tIntervention: {}'.format(\n",
    "                        batch_idx * len(X4), len(loader.dataset), 100. * batch_idx / len(loader), intervention))\n",
    "                    \n",
    "                Counterfactuals['X_6'].append(np.array(samples['X_6']))\n",
    "                Counterfactuals['X_5'].append(np.array(samples['X_5']))\n",
    "                Counterfactuals['X_4'].append(np.array(samples['X_4']))\n",
    "                Counterfactuals['X_3'].append(np.array(samples['X_3']))\n",
    "                Counterfactuals['X_2'].append(np.array(samples['X_2'])) \n",
    "                Counterfactuals['X_1'].append(np.array(samples['X_1']))\n",
    "                Counterfactuals['Y'].append(np.array(samples['Y']))\n",
    "    print(' Time taken: {:.2f} secs'.format( (time.time()-start_time)))\n",
    "     \n",
    "    Counterfactuals['X_6']=np.vstack(Counterfactuals['X_6']).reshape(sample_size)\n",
    "    Counterfactuals['X_5']=np.vstack(Counterfactuals['X_5']).reshape(sample_size)            \n",
    "    Counterfactuals['X_4']=np.vstack(Counterfactuals['X_4']).reshape(sample_size)\n",
    "    Counterfactuals['X_3']=np.vstack(Counterfactuals['X_3']).reshape(sample_size)\n",
    "    Counterfactuals['X_2']=np.vstack(Counterfactuals['X_2']).reshape(sample_size)\n",
    "    Counterfactuals['X_1']=np.vstack(Counterfactuals['X_1']).reshape(sample_size)\n",
    "    Counterfactuals['Y']=np.vstack(Counterfactuals['Y']).reshape(sample_size)  \n",
    "    \n",
    "    return Counterfactuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e66138-7757-41d1-b973-baf60c54f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "dim=1\n",
    "particles=32\n",
    "log_interval=100\n",
    "save='logs2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd5e63-2b65-4398-928d-6700729e28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "j= 10\n",
    "intervention='do(X5='+str(j)+')'\n",
    "X_6,X_5, X_4, X_3, X_2, X_1, Y= SCM(True,j)      #True Counterfactuals\n",
    "np.mean(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7d0c3-0291-4a2c-ba40-4722154c38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCM_counterfactuals=do_inference(scm,intervention)\n",
    "np.mean(SCM_counterfactuals['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02268d1-ceda-4936-94b1-8fcdf8fcb7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCM_partial_counterfactuals=do_inference(scm_partial,intervention)\n",
    "np.mean(SCM_partial_counterfactuals['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf2db47-a07b-423c-9ebb-df4987c58e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "data={'Y':Y,'partial':SCM_partial_counterfactuals['Y'],'full':SCM_counterfactuals['Y']}\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.kdeplot(data['Y'], ax=ax, color='red', fill=True,bw_adjust=1,legend=True,label='True',alpha=0.1)\n",
    "sns.kdeplot(data['full'], ax=ax, color='yellow',linestyle='-' ,fill=False,bw_adjust=1,legend=True,label='full model')\n",
    "sns.kdeplot(data['partial'], ax=ax,color='black',linestyle='--' ,fill=False,bw_adjust=1,legend=True,label='partial model')\n",
    "plt.legend()\n",
    "plt.ylabel('Density',fontweight='bold')\n",
    "plt.title(r'$P(Y_{X_{5}\\leftarrow %.2f}|\\mathbf{X}=\\mathbf{x}^{obs},Y=y^{obs})$'%j,fontsize=15)\n",
    "#fig.savefig('toy_model.pdf',format='pdf',dpi=600)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
